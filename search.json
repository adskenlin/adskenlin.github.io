[{"title":"Awesome Paper (1)","url":"/2021/10/03/Awesome-Paper-(1)/","content":"To be updated."},{"title":"Deploy PyTorch Model with TensorRT","url":"/2021/10/03/Deploy-PyTorch-Model-with-TensorRT/","content":"To be updated.","tags":["TensorRT","PyTorch"]},{"title":"Basic and Advanced Git","url":"/2021/10/01/Basic-and-Advanced-Git/","content":"**Git** is version-control tool for tracking changes in files. It is widely used in software development project and It significantly increases the working efficient among teams and supports for distributed, non-linear workflows (thousands of parallel branches running on different systems). One thing more to mention, it was created by Linus Torvals (Genius!!!).\n\nThis Blog is written for marking down the key points and tricks of git. It will keep updating!\n\n## Installation\n\nFollow the official websites to install git: https://git-scm.com/book/en/v2/Getting-Started-Installing-Git\n\nTo verify the installation, use `git --version` in terminal, you should see the output `git version x.xx.x`.\n\n## Setup\n\n```bash\n# Set up git configs\n\n# 1. Identity\ngit config --global user.name \"YOUR NAME\"\ngit config --global user.email \"YOUR EMAIL\"\n# 2. Default Branch Name\ngit config --global init.defaultBranch main\n# All Info\ngit config --list \n```\n\n## Tips and Tricks\n\n1. If you want to create a git repository at local, try:\n\n   ```bash\n    cd YOUR_REPO\n    git init\n   ```\n\n2. If you want to clone a git repo from remote(github, gitlab, etc.), try:\n\n   ```bash\n   git clone YOUR_URL\n   ```\n\n3. The files in Git system could have 4 types: **Untracked**, **Unmodified**, **Modified**, **Staged**. \n\n4. Check the status of all files:\n\n   ```bash\n   git status\n   # output:\n   On branch master\n   Your branch is up-to-date with 'origin/master'.\n   Changes to be committed:\n     (use \"git reset HEAD <file>...\" to unstage)\n   \n       new file:   README\n       modified:   CONTRIBUTING.md\n   \n   Changes not staged for commit:\n     (use \"git add <file>...\" to update what will be committed)\n     (use \"git checkout -- <file>...\" to discard changes in working directory)\n   \n       modified:   CONTRIBUTING.md\n   ```\n\n   As above shown, `git status` will output the current branch, files to be commited, files to add on stage, etc.\n\n5. If you have some files that you do not want Git to add or track, use file named `.gitignore`, add the name of files.\n\n6. View Changes:\n\n   ```bash\n   git diff\n   git diff --staged\n   ```\n\n7. Commit:\n\n   ```bash\n   # open edit and commit\n   git commit\n   \n   # one-line commit\n   git commit -m \"YOUR COMMIT\"\n   \n   # skip add, one-line commit\n   git commit -a -m \"YOUR COMMIT\"\n   ```\n\n8. Remove files: `git rm FILE`\n\n9. Rename files (actually use move operation):\n\n   ```bash\n   git mv old_file new_file\n   ```\n\n10. View commit history:\n\n    ```bash\n    # normal log\n    git log\n    # limit the log content to n(2) lines\n    git log -p -2\n    # limit the log content w.r.t. time\n    git log --since=2.weeks\n    # show statistc of log\n    git log --stat\n    # specific formats for log\n    git log --pretty=oneline\n    git log --pretty=format:\"%h %s\" --graph\n    # Very specific\n    git log --pretty=\"%h - %s\" --author='Junio C Hamano' --since=\"2008-10-01\" \\\n       --before=\"2008-11-01\" --no-merges -- t/\n    ```\n\n11. Undoing Things:\n\n    ```bash\n    # when you commit too early,\n    # still want to add some files then commit\n    git commit --amend\n    ```\n\n12. Unstaging a staged file:\n\n    ```bash\n    git status\n    # output:\n    On branch master\n    Changes to be committed:\n      (use \"git reset HEAD <file>...\" to unstage)\n    \n        renamed:    README.md -> README\n        modified:   CONTRIBUTING.md\n    git reset HEAD ...\n    ```\n\n13. Unmodifying a modified file:\n\n    ```bash\n    git status\n    # output:\n    Changes not staged for commit:\n      (use \"git add <file>...\" to update what will be committed)\n      (use \"git checkout -- <file>...\" to discard changes in working directory)\n    \n        modified:   CONTRIBUTING.md\n    git checkout -- xxx.md\n    ```\n\n14. `git restore`\n\n    ```bash\n    # unstage a staged file\n    git restore --staged <file>\n    # unmodifying a modified file\n    git restore <file>\n    ```\n\n15. Working with Remotes:\n\n    ```bash\n    # show remotes\n    git remote -v\n    # add remotes\n    git remote add <shortname> <url>\n    # Fetching and Pulling\n    git fetch <remote>\n    git push <remote> <branch>\n    # show remote information\n    git remote show origin\n    # rename and remove remotes\n    git remote rename <name1> <name2>\n    git remote rm <name>\n    ```\n\n16. Working with Tags:\n\n    ```bash\n    # list tags\n    git tag\n    git tag -l \"version-*\"\n    # create annotated tags\n    git tag -a version -m \"commit for version\"\n    # create lightweight tags\n    git tag version\n    # show tag\n    git show tag_name\n    # add tag to an old version\n    git tag -a version LOG_NUM\n    # git push will not transfer tags to remote servers by default\n    git push origin <tagname>\n    git push origin --tags\n    # delete tag\n    git tag -d <tagname>\n    # check out tags\n    git checkout <tagname>\n    # be careful with:\n    git checkout -b <branch> <tagname>\n    ```\n\n17. Aliases:\n\n    ```bash\n    git config --global alias.last 'log -1 HEAD'\n    git last\n    ```\n\n18. BASIC Branches:\n\n    ```bash\n    # create branch (but will not switch to it)\n    git branch <BranchName>\n    # show the branch pointers info\n    git log --oneline --decorate\n    # switch branches\n    git checkout <BranchName>\n    # After some comments, the HEAD will move forward\n    # move HEAD back to master\n    git checkout master\n    # show divergent history\n    git log --oneline --decorate --graph --all\n    \n    \n    # create a new branch and switch\n    git checkout -b <BranchName>\n    # merge a branch, master will fast-forward to BranchName\n    git checkout master\n    git merge <BranchName>\n    # then delete the BranchName because no more needed\n    git branch -d <BranchName>\n    \n    \n    # merge conflicts\n    git merge <BranchName>\n    # output:\n    Auto-merging index.html\n    CONFLICT (content): Merge conflict in index.html\n    Automatic merge failed; fix conflicts and then commit the result.\n    git status\n    # ..., resolve the conflicts\n    # another graphical tool: git mergetool?\n    ```\n\n19. Branches MANAGEMENT:\n\n    ```bash\n    # about git branch\n    # see all branches and last commit on each branch\n    git branch -v\n    # about merged/unmerged branch\n    git branch --merged\n    git branch --no-merged\n    \n    # rename branch\n    git branch --move old-name new-name\n    git push --set-upstream origin new-name\n    # but now the old-name still exists at remote\n    # to delete\n    git push origin --delete old-name\n    \n    \n    \n    ```\n\n20. REMOTE Branches:\n\n    ```bash\n    # REMOTE: R0 <- R1 <- R2 \n    #                     |\n    #                    master\n    #\n    #             origin/master (remote branch)\n    #                    |\n    # LOCAL: R0 <- R1 <- R2\n    #                    |\n    #                   master (local branch)\n    ```\n\n    ```bash\n    # REMOTE: R0 <- R1 <- R2 <- R3 <- R4\n    #                                 |\n    #                               master\n    #\n    #             origin/master (remote branch)\n    #                    |\n    # LOCAL: R0 <- R1 <- R2 <- L3 <- L4\n    #                                |\n    #                              master (local branch)\n    ```\n\n    ```bash\n    # git fetch <remote>\n    git fetch origin\n    ```\n\n    ```bash\n    # After FETCH:\n    #\n    #                        origin/master (remote branch)\n    #                                |\n    # LOCAL: R0 <- R1 <- R2 <- R3 <- R4\n    #                    |\n    #                    <- L3 <- L4\n    #                             |\n    #                        master (local branch)\n    ```\n\n    ```bash\n    # IF THERE IS ANOTHER REMOTE:\n    # REMOTE_1: R0 <- R1 <- R2 <- R3 <- R4\n    #                       |\n    #                     master\n    \n    git fetch REMOTE_1\n    \n    #           REMOTE_1/master     origin/master (REMOTE)\n    #                    |           |\n    # LOCAL: R0 <- R1 <- R2 <- R3 <- R4\n    #                    |\n    #                    <- L3 <- L4\n    #                             |\n    #                        master (local branch)\n    ```\n\n    ```bash\n    # Push:\n    git push <remote> <LocalBranch>:<RemoteBranch>\n    # or if you keep the name of branches\n    git push <remote> <Branch>\n    \n    # Tracking Branches and automatically pull from remote:\n    # create a new one, switch and track\n    git checkout --track origin/branchname\n    # use the local one to track\n    git branch -u origin/branchname\n    \n    # fetch all and show\n    git fetch --all; git branch -vv\n    \n    ```\n\n21. Rebasing:\n\n    ```bash\n    # Integrate changes from one to another\n    # similar to MERGE\n    # if we have two divergent work: master, experiment\n    # switch to experiment\n    git checkout experiment\n    # integrate experiment as the node next to master\n    git rebase master\n    # switch to master\n    git checkout master\n    # merge\n    git merge experiment\n    ```\n\n    ```bash\n    # There are 3 branches: master, server, client\n    \n    git rebase --onto master server client\n    git checkout master\n    git merge client\n    # git rebase <basebranch> <topicbranch>\n    git rebase master server\n    git checkout master\n    git merge server\n    git branch -d client\n    git branch -d server\n    ```\n\n## Reference:\n\n1. 13 Advanced (but useful) Git Technologies and Shortcuts https://www.youtube.com/watch?v=ecK3EnyGD8o\n2. Git Docs https://git-scm.com/book/en/v2\n3. Git Wiki https://en.wikipedia.org/wiki/Git\n\n","tags":["Git"]},{"title":"MMDetection3D: Deep Learning Toolbox for 3D LiDAR Data - P2","url":"/2021/09/08/MMDetection3D-Deep-Learning-Toolbox-for-3D-LiDAR-Data-P2/"},{"title":"MMDetection3D: Deep Learning Toolbox for 3D LiDAR Data - P1","url":"/2021/09/07/MMdetection3D-Deep-Learning-Codebase-for-3D-LiDAR-Data/","content":"MMDetection3D is an open-source object detection codebase tool based on PyTorch. It's a project developed by MMLab, which has published several famous toolboxes e.g. MMDetection, MMsegmentation and so on. \n\nFor more information, please visit their [official github repo](https://github.com/open-mmlab/mmdetection3d). In this blog, I would like to introduce this toolbox and explain how it works from its source code.\n\n## Introduction\n\nThis toolbox was built focusing on 3D object detection tasks. If you've heard about `mmdetection` of Openmmlab, you could think this repo as an 3D-extension of basic `mmdetection`. It is built based on `mmdetection` and `mmcv`, and it use nearly same mechanism and coding-style as `mmdetection`.\n\nI would highly recommend this toolbox for 3D Object Detection, since it has everything we need in this field and it works much more efficient than your own hand-written codes. Reasons:\n\n+ It provides functions and built-in dataset classes to process raw 3D Data no matter it's from NuScenes, Lyft or Waymo.\n\n+ It provides a huge number of functions from SOTA methods for processing point cloud data, anchors, bounding boxes, which we may use during our pre-processing step.\n\n+ It contains various operations and their functions, which we may use in different parts of network, such as spare 3D convolution, iou3D calculation.\n\n+ while designing architecture, it provides us various sub-architectures for ablation study. e.g. in different parts of network, such as in `backbone`, `Neck`, `Head`, it provides the architectures from many SOTA methods.\n\n## Installation\n\nFollow the steps [here](https://github.com/open-mmlab/mmdetection3d/blob/master/docs/getting_started.md). One thing you need to pay attention is, that the version of mmcv, python, pytorch and mmdetection should match. I was stucking in a problem for a long time, which causes the last installtion step `pip install -v -e .` failed. Because I have multiple CUDA versions(e.g 10.1, 11.2) installed and it does not match the pytorch version I used.\n\n## Look into Codes\n\nYou could see many folders in `/mmdetection3d`. I would like sort them in 3 levels:\n\n+ Sure to use: `configs`, `mmdet3d`,  `tools`\n\n+ May use: `data`, `tests`\n\n+ not relevant: `others`\n\n### What is Configs?\n\nIn `configs`, you will have to create the config file for your deep learning methods, which should include the information of dataset setup, neural network architecture, training and validation parameters optimization and so on.\n\nThe basic mechanism of this toolbox is, that it has set up all the definitions, parameters in its registry module. When you want to use one of them, you only have to write it in config file and it will be called up automatically. \n\nTo explain it clearly, let's look into an example, Centerpoint: \n\n```python\n\n# read basic config files of different parts\n# for dataset, use nus-3d.py as default\n# for network architecture, use centerpoint_01voxel_second_secfpn_nus.py as default\n# for training params and runtime setup, use cyclic_20e.py and default_runtime.py as default\n_base_ = [\n    '../_base_/datasets/nus-3d.py',\n    '../_base_/models/centerpoint_01voxel_second_secfpn_nus.py',\n    '../_base_/schedules/cyclic_20e.py', '../_base_/default_runtime.py'\n]\n\n# In the following part you have to set up the parameters\n# which should overwrite the default values\n\n# in this part,\n# point_cloud_range and class_names are common params\n# they may be used not only in dataset, \n# and also in models and training loops\npoint_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]\nclass_names = [\n    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',\n    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'\n]\n\n\n# in this part,\n# the params for setting up a model will be partially overwritten\nmodel = dict(\n    pts_voxel_layer=dict(point_cloud_range=point_cloud_range),\n    pts_bbox_head=dict(bbox_coder=dict(pc_range=point_cloud_range[:2])),\n    train_cfg=dict(pts=dict(point_cloud_range=point_cloud_range)),\n    test_cfg=dict(pts=dict(pc_range=point_cloud_range[:2])))\n\n\n# in this part,\n# there are definitions about dataset, data path,\n# database sampler, preprocessing pipeline for training and testing,\n# they are necessary for setting up a dataset object\ndataset_type = 'NuScenesDataset'\ndata_root = 'data/nuscenes/'\nfile_client_args = dict(backend='disk')\n\ndb_sampler = dict(\n    data_root=data_root,\n    info_path=data_root + 'nuscenes_dbinfos_train.pkl',\n    rate=1.0,\n    prepare=dict(\n        filter_by_difficulty=[-1],\n        filter_by_min_points=dict(\n            car=5,\n            truck=5,\n            bus=5,\n            trailer=5,\n            construction_vehicle=5,\n            traffic_cone=5,\n            barrier=5,\n            motorcycle=5,\n            bicycle=5,\n            pedestrian=5)),\n    classes=class_names,\n    sample_groups=dict(\n        car=2,\n        truck=3,\n        construction_vehicle=7,\n        bus=4,\n        trailer=6,\n        barrier=2,\n        motorcycle=6,\n        bicycle=6,\n        pedestrian=2,\n        traffic_cone=2),\n    points_loader=dict(\n        type='LoadPointsFromFile',\n        coord_type='LIDAR',\n        load_dim=5,\n        use_dim=[0, 1, 2, 3, 4],\n        file_client_args=file_client_args))\n\ntrain_pipeline = [\n    dict(\n        type='LoadPointsFromFile',\n        coord_type='LIDAR',\n        load_dim=5,\n        use_dim=5,\n        file_client_args=file_client_args),\n    dict(\n        type='LoadPointsFromMultiSweeps',\n        sweeps_num=9,\n        use_dim=[0, 1, 2, 3, 4],\n        file_client_args=file_client_args,\n        pad_empty_sweeps=True,\n        remove_close=True),\n    dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),\n    dict(type='ObjectSample', db_sampler=db_sampler),\n    dict(\n        type='GlobalRotScaleTrans',\n        rot_range=[-0.3925, 0.3925],\n        scale_ratio_range=[0.95, 1.05],\n        translation_std=[0, 0, 0]),\n    dict(\n        type='RandomFlip3D',\n        sync_2d=False,\n        flip_ratio_bev_horizontal=0.5,\n        flip_ratio_bev_vertical=0.5),\n    dict(type='PointsRangeFilter', point_cloud_range=point_cloud_range),\n    dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),\n    dict(type='ObjectNameFilter', classes=class_names),\n    dict(type='PointShuffle'),\n    dict(type='DefaultFormatBundle3D', class_names=class_names),\n    dict(type='Collect3D', keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])\n]\ntest_pipeline = [\n    dict(\n        type='LoadPointsFromFile',\n        coord_type='LIDAR',\n        load_dim=5,\n        use_dim=5,\n        file_client_args=file_client_args),\n    dict(\n        type='LoadPointsFromMultiSweeps',\n        sweeps_num=9,\n        use_dim=[0, 1, 2, 3, 4],\n        file_client_args=file_client_args,\n        pad_empty_sweeps=True,\n        remove_close=True),\n    dict(\n        type='MultiScaleFlipAug3D',\n        img_scale=(1333, 800),\n        pts_scale_ratio=1,\n        flip=False,\n        transforms=[\n            dict(\n                type='GlobalRotScaleTrans',\n                rot_range=[0, 0],\n                scale_ratio_range=[1., 1.],\n                translation_std=[0, 0, 0]),\n            dict(type='RandomFlip3D'),\n            dict(\n                type='PointsRangeFilter', point_cloud_range=point_cloud_range),\n            dict(\n                type='DefaultFormatBundle3D',\n                class_names=class_names,\n                with_label=False),\n            dict(type='Collect3D', keys=['points'])\n        ])\n]\n\neval_pipeline = [\n    dict(\n        type='LoadPointsFromFile',\n        coord_type='LIDAR',\n        load_dim=5,\n        use_dim=5,\n        file_client_args=file_client_args),\n    dict(\n        type='LoadPointsFromMultiSweeps',\n        sweeps_num=9,\n        use_dim=[0, 1, 2, 3, 4],\n        file_client_args=file_client_args,\n        pad_empty_sweeps=True,\n        remove_close=True),\n    dict(\n        type='DefaultFormatBundle3D',\n        class_names=class_names,\n        with_label=False),\n    dict(type='Collect3D', keys=['points'])\n]\n\ndata = dict(\n    train=dict(\n        type='CBGSDataset',\n        dataset=dict(\n            type=dataset_type,\n            data_root=data_root,\n            ann_file=data_root + 'nuscenes_infos_train.pkl',\n            pipeline=train_pipeline,\n            classes=class_names,\n            test_mode=False,\n            use_valid_flag=True,\n            box_type_3d='LiDAR')),\n    val=dict(pipeline=test_pipeline, classes=class_names),\n    test=dict(pipeline=test_pipeline, classes=class_names))\n\n\n# In this part,\n# some params in runtime or training loop will be overwritten\nevaluation = dict(interval=20, pipeline=eval_pipeline)\n```\n\nTo draw a conclusion, the `configs` is where we define our network. We could use pre-defined definition or built-in functions to build up our neural network. If you think up a parameter or architecture which does not exist in toolbox, you could also registry them in next part `mmdet3d`.\n\n### What is mmdet3d?\n\n`mmdet3d` is the place where you could find and fetch your screwdriver. Under it, there are: `apis`, `core`, `dataset`, `models`, `ops`, `utils`.\n\n+ In `apis` you could find the simple api-codes for training, testing and inference. \n\n+ In `core` you could find many pre-defined tools, which may be helpful in your research. E.g. `Anchor3DRangeGenerator` defines the method object to generate anchors. (for anchor-based methods, anchors could be generated in different ways)  \n\n+ In `dataset` and `models` it provides various dataset/model options for your neural network. E.g. for models, you could choose ResNet as `backbone`, FPN as `neck`, anchor3dhead as `tast_heads`, and combine them as your network architecture.\n\n+ In `ops` it provides many functions for data processing. E.g. Sparse 3D Convolution as an option of processing method for voxelized space.\n\nIf you want to design your own methods or architectures, you could just put the codes in corresponding folders and add your design to registry module. The toolbox could then fetch your self-designed object easily.\n\n### What is tools?\n\nIn `tools`, it provides us codes for creating database, training, testing, parallel training with multi-GPUs or multi-workstations and so on. \n\nBriefly speaking, you'll set up all parameters and arguments of your neural network and training loop in `configs`, if you have some self-designed methods and architectures, you have to put and regitry them in `mmdet3d`. Afterwards you could run training and evaluation process with the codes in `tools`.\n\n## What's coming NEXT\n\nIn my next blog 'MMDetection3D: Deep Learning Codebase for 3D LiDAR Data - P2', we will introduce the details of training in MMDetection3D.","tags":["Deep Learning","LiDAR","Point Cloud Data"]},{"title":"Deep Learning on 3D LiDAR Data P2 Pre-Processing","url":"/2021/09/07/Deep-Learning-Deep-Learning-on-LiDAR-Data-P2-Pre-Processing/","content":"\n# P2 - Data Pre-processing\n\nIn P1 we've known the data formats and structures in NuScenes Dataset. In addition, many functions are provided for processing information from the dataset. In this part, we will introduce some methods to re-organise and process the data, in order to make it useable for our deep learning network.\n","tags":["NuScenes","Deep Learning","LiDAR"]},{"title":"Deep Learning on 3D LiDAR Data P1 Dataset","url":"/2021/08/16/Deep-Learning-on-NuScenes-LiDAR-Data/","content":"\n# P1 - NuScenes Dataset Introduction\n\n## Why NuScenes?\n\nWith increasing inverstment in fields of autonomous driving, tech-companies are creating their own dataset for training their autonomous vehicles. Some of them, like Waymo Dataset, Lyft L5 Dataset and NuScenes Data are widely used in personal uncommercial reasearchs because of their open-source and mutimodal features.\n\nNuScenes Dataset as one of the earliest published dataset in this field contains various kinds of data collected by different sensors such as cameras, radar and LiDARs. It provides toolkit to help researchers easily and quickly get an overview and process the data with provided functions. And Lyft L5 Dataset also uses similiar toolkit.\n\nThus, let's start with processing NuScenes Dataset. Since we are focusing on deep learning with LiDAR data, which means we have to prepar the LiDAR data for our deep learning neural network, we'll first take an overview of the most important factors of dataset:\n\n+ Size\n+ Variety of Scenes\n+ Number and Quality of Annotated Objects\n\n|Dataset|Scenes|Size|PCs lidar|Ann.Frames|3D boxes|\n|----|----|----|----|----|----|\n|NuScenes|1k|5.5hr|400k|40k|1.4M|\n|Lyft L5|366|2.5hr|46k|46k|1.3M|\n|Waymo Open|1k|5.5hr|200k|200k|12M|\n\nAs the table shown above, NuScenes has a larger data size comparing to Lyft L5, and has an easy-to-use toolkit comparing to Waymo Dataset. For training a neural network, the more data we have, the better performance we could achieve. And with more different driving scenes and more different objects classes, the neural network will become robuster after training.\n\n## Toolkit(Devkit) Installation\n\nDevkit provides us various functions to extract the specific data format and data information from dataset. It also contains different matricies for evaluating NN(Neural Network)'s performance with respect to your goals such as prediction, detecton and so on.\n\n+ visit [official website](https://www.nuscenes.org/) and account registration\n+ Download Full dataset(v1.0): Trainval and Test Set\n+ Devkit Installation (for more information please visit their [github](https://github.com/nutonomy/nuscenes-devkit))\n\n> ```bash\n> # under Ubuntu or MacOS\n> # if you do not have python 3.6/3.7\n> # install python first\n> sudo apt install python-pip\n> sudo add-apt-repository ppa:deadsnakes/ppa\n> sudo apt-get update\n> sudo apt-get install python3.7\n> sudo apt-get install python3.7-dev\n> # then create virtual environment via conda or virtualenvwrapper\n> # if you do not have miniconda, google and install it\n> # after miniconda installed\n> conda create --name deep-learning-nuscenes python=3.7\n> conda activate deep-learning-nuscenes\n> # to deactivate env, use `source deactivate`\n> # then use\n> pip install nuscenes-devkit\n> ```\n\n+ Verify the installation and follow the tutorials [here](https://www.nuscenes.org/nuscenes?tutorial=nuscenes).\n\n## What could we do with Dev-Kit?\n\nWith devkit of nuscenes we could get an overview of the whole dataset and understand the data formats and structure inside the dataset. We would also use the functions, which devkit provides us, in future steps to build a dataset pre-processing pipeline.\n\nAs the introduction, let us try some in jupyter notebook to get better understand of the data formats of the dataset.\n\nTo receive an overview of the dataset:\n\n```python\nfrom nuscenes.nuscenes import NuScenes\n# if you download the mini-dataset\n# use version=\"v1.0-mini\"\n# dataroot should be the path\n# where you store your dataset\nnusc = NuScenes(version = \"v1.0-trainval\", dataroot=\"/home/ken/Data/Dataset/NuScenes\")\n\n# output:\n# ======\n# Loading NuScenes tables for version v1.0-trainval...\n# 23 category,\n# 8 attribute,\n# 4 visibility,\n# 64386 instance,\n# 12 sensor,\n# 10200 calibrated_sensor,\n# 2631083 ego_pose,\n# 68 log,\n# 850 scene,\n# 34149 sample,\n# 2631083 sample_data,\n# 1166187 sample_annotation,\n# 4 map,\n# Done loading in 37.807 seconds.\n# ======\n# Reverse indexing ...\n# Done reverse indexing in 6.4 seconds.\n# ======\n```\n\nAs above shown, we could see there are several data types in this dataset, e.g. `category`, `attribute`, `visibility`, `instance`, `sensor`, `calibrated_sensor`, `ego_pose`, `scene`, ... and so on. You can find the official definition [here](https://www.nuscenes.org/nuscenes#data-format).\n\nFor our focus, `scene`, `sample`, `sample_data`, `sample_annotation` will be important.\n\n1. scene\n\n   To get information of scenes:\n\n   ```python\n   nusc.list_scenes()\n   # output:\n   # scene-0161, Car overtaking, parking lot, peds, ped ... [18-05-21 15:07:23]   19s, boston-seaport, anns:1970\n   # scene-0162, Leaving parking lot, parked cars, hidde... [18-05-21 15:07:43]   19s, boston-seaport, anns:2230\n   # ...\n   ```\n\n   Watching these outputs, we could know the scenes contain different driving scenarios and each scene lasts about 20 seconds.\n\n   For a specific scene:\n\n   ```python\n   my_scene = nusc.scene[0]\n   my_scene\n   # output:\n   # {'token': '73030fb67d3c46cfb5e590168088ae39',\n   # 'log_token': '6b6513e6c8384cec88775cae30b78c0e',\n   # 'nbr_samples': 40,\n   # 'first_sample_token': 'e93e98b63d3b40209056d129dc53ceee',\n   # 'last_sample_token': '40e413c922184255a94f08d3c10037e0',\n   # 'name': 'scene-0001',\n   # 'description': 'Construction, maneuver between several trucks'}\n   \n   # explanation:\n   # 'token' -- Code for searching and calling this scene\n   # 'log_token' -- Code for searching and calling this scene's log\n   # 'nbr_samples' -- number of samples in this scene\n   # 'first_sample_token/last_sample_token' -- Code for searching and calling this scene's first/last frame\n   # 'name', 'decription' -- other information\n   ```\n\n2. sample\n\n   A sample means a frame. In this dataset, the data is collected every 0.1 second(10 Hz), and it is annotated every half a second(2 Hz), which means every 5 frames, we would have 1 annotated sample. In annotated sample, the existed objects will be annotated.\n\n   Let us take a sample as example:\n\n   ```python\n   first_sample_token = my_scene['first_sample_token']\n   my_sample = nusc.get('sample', first_sample_token)\n   my_sample\n\n   # output:\n   # {'token': 'e93e98b63d3b40209056d129dc53ceee',\n   # 'timestamp': 1531883530449377,\n   # 'prev': '',\n   # 'next': '14d5adfe50bb4445bc3aa5fe607691a8',\n   # 'scene_token': '73030fb67d3c46cfb5e590168088ae39',\n   # 'data': {'RADAR_FRONT': 'bddd80ae33ec4e32b27fdb3c1160a30e',\n   # 'RADAR_FRONT_LEFT': '1a08aec0958e42ebb37d26612a2cfc57',\n   # 'RADAR_FRONT_RIGHT': '282fa8d7a3f34b68b56fb1e22e697668',\n   # 'RADAR_BACK_LEFT': '05fc4678025246f3adf8e9b8a0a0b13b',\n   # 'RADAR_BACK_RIGHT': '31b8099fb1c44c6381c3c71b335750bb',\n   # 'LIDAR_TOP': '3388933b59444c5db71fade0bbfef470',\n   # 'CAM_FRONT': '020d7b4f858147558106c504f7f31bef',\n   # 'CAM_FRONT_RIGHT': '16d39ff22a8545b0a4ee3236a0fe1c20',\n   # 'CAM_BACK_RIGHT': 'ec7096278e484c9ebe6894a2ad5682e9',\n   # 'CAM_BACK': 'aab35aeccbda42de82b2ff5c278a0d48',\n   # 'CAM_BACK_LEFT': '86e6806d626b4711a6d0f5015b090116',\n   # 'CAM_FRONT_LEFT': '24332e9c554a406f880430f17771b608'},\n   # 'anns': ['173a50411564442ab195e132472fde71',\n   # '5123ed5e450948ac8dc381772f2ae29a',\n   # 'acce0b7220754600b700257a1de1573d',\n   # '8d7cb5e96cae48c39ef4f9f75182013a',\n   # 'f64bfd3d4ddf46d7a366624605cb7e91',\n   # 'f9dba7f32ed34ee8adc92096af767868',\n   # '086e3f37a44e459987cde7a3ca273b5b',\n   # '3964235c58a745df8589b6a626c29985',\n   # '31a96b9503204a8688da75abcd4b56b2',\n   # 'b0284e14d17a444a8d0071bd1f03a0a2']}\n\n   # explanation:\n   # 'token' -- Code for searching and calling this sample\n   # 'timestamp' -- the timestamp of this frame\n   # 'prev'/'next' -- the frame of previous/next frame(before/after 0.1 sec).\n   # `scene` -- the token of scene, which this sample belongs to.\n   # `data` -- the token of data, which different sensors collect at this time frame, we could see different radar, lidar and cameras\n   # `anns` -- the token of annotated objects in this frame\n   ```\n\n3. sample_data\n\n   sample data means the data collected by a specific sensor in a frame. In part of sample, we could know the dataset contains data collected by RADAR, LiDAR, and Cameras. Let us take look how lidar data looks like:\n\n   ```python\n   sensor = 'LIDAR_TOP'\n   lidar_data = nusc.get('sample_data', my_sample['data'][sensor])\n   lidar_data\n\n   # output:\n   # {'token': '3388933b59444c5db71fade0bbfef470',\n   # 'sample_token': 'e93e98b63d3b40209056d129dc53ceee',\n   # 'ego_pose_token': '3388933b59444c5db71fade0bbfef470',\n   # 'calibrated_sensor_token': '7a0cd258d096410eb68251b4b87febf5',\n   # 'timestamp': 1531883530449377,\n   # 'fileformat': 'pcd',\n   # 'is_key_frame': True,\n   # 'height': 0,\n   # 'width': 0,\n   # 'filename': 'samples/LIDAR_TOP/n015-2018-07-18-11-07-57+0800__LIDAR_TOP__1531883530449377.pcd.bin',\n   # 'prev': '',\n   # 'next': 'bc2cd87d110747cd9849e2b8578b7877',\n   # 'sensor_modality': 'lidar',\n   # 'channel': 'LIDAR_TOP'}\n\n   # explanation:\n   # a set of 'token' -- code for corresponding object\n   # 'timestamp' -- the timestamp of collection\n   # 'fileformat' -- here the lidar data is in point cloud format\n   # 'is_key_frame' -- keyframe means the frame is annotated\n   # 'heigt'/'width' -- 3D lidar data does not have these attributes\n   # 'filename' -- path of the stored data\n   # 'prev'/'next' -- token of data in previous/next frame\n\n   ```\n\n   For visualization of the lidar data:\n\n   ```python\n   nusc.render_sample_data(lidar_data['token'])\n   ```\n\n   ![](https://raw.githubusercontent.com/adskenlin/adskenlin.github.io/master/2021/08/16/Deep-Learning-on-NuScenes-LiDAR-Data/lidar_vis.png)\n\n4. sample_annotation\n\n   sample_annotation refers to the anntated information of an object in a sample.\n\n   ```python\n   my_annotation_token = my_sample['anns'][5]\n   my_annotation_metadata =  nusc.get('sample_annotation', my_annotation_token)\n   my_annotation_metadata\n\n   # output:\n   # {'token': 'f9dba7f32ed34ee8adc92096af767868',\n   # 'sample_token': 'e93e98b63d3b40209056d129dc53ceee',\n   # 'instance_token': '076e76a589dd4f40adce27b3f3377f58',\n   # 'visibility_token': '4',\n   # 'attribute_tokens': ['cb5118da1ab342aa947717dc53544259'],\n   # 'translation': [1009.009, 598.528, 0.664],\n   # 'size': [1.871, 4.478, 1.456],\n   # 'rotation': [0.8844059033120215, 0.0, 0.0, 0.46671854279302766],\n   # 'prev': '',\n   # 'next': '21ece7170dfa431bb504e15f68fc40ce',\n   # 'num_lidar_pts': 151,\n   # 'num_radar_pts': 3,\n   # 'category_name': 'vehicle.car'}\n\n   # explanation:\n   # a set of token -- code for corresponding object\n   # 'translation' -- the global coordinate system of center of this object\n   # 'size' -- the size(width, length, height) of this object\n   # 'rotation' -- the orientation of the object shown in quaternion\n   # 'num_lidar_pts' -- number of lidar points on this object\n   # 'category_name' -- object class name \n   ```\n\n   For visulization of this object:\n\n   ```python\n   nusc.render_annotation(my_annotation_token)\n   ```\n\n   ![](https://raw.githubusercontent.com/adskenlin/adskenlin.github.io/master/2021/08/16/Deep-Learning-on-NuScenes-LiDAR-Data/anns_vis.png)\n\n## Data Format and Usage\n\nAs above shown, we've introduced different data formats in NuScenes dataset. The data formats and their corresponding functions could help us build data processing pipeline in future steps.\n\nBefore we build the data pre-processing pipeline, we need to think about: What kind of LiDAR data do we need for deep learning? An example of senor data processing from SOTA methods like Center-Net, MultiXNet shows the pipeline as follows: \n\n![](https://raw.githubusercontent.com/adskenlin/adskenlin.github.io/master/2021/08/16/Deep-Learning-on-NuScenes-LiDAR-Data/lidardata_pipeline.PNG)\n\nWe'll discuss data pre-processing methods and options in next part.\n\n## Reference\n\n[1] [nuScenes: A multimodal dataset for autonomous driving](https://arxiv.org/pdf/1903.11027.pdf)\n\n[2] [Center-based 3D Object Detection and Tracking](https://arxiv.org/pdf/2006.11275.pdf)\n","tags":["NuScenes","Deep Learning","LiDAR"]},{"title":"Solution to Problems You May Meet With Parallel System Ubuntu","url":"/2021/08/15/Solution-to-Problems-You-May-Meet-With-Parallel-System-Linux/","content":"This article collects the problems I met while I was installing Ubuntu and using Ubuntu at the beginning.\n## Normal Steps to Install Linux(Ubuntu)\n+ Download Ubuntu from [official website](https://ubuntu.com/download/desktop).\n+ Burn the .msi to your U-Disk with [Rufus](http://rufus.ie/en/).\n+ Prapare the Disk partition for Ubuntu System:\nWindows' Disk Management -> Right Click on your available Disk -> Shrink Volume... -> Set the space -> Shrink\n+ Turn off Win10 Quick Start\n+ Restart the PC, stick your U-Disk, go to BIOS Setting\n+ Choose \"UEFI: USB Flash Disk\" in start menu\n+ Choose \"Install Ubuntu\"\n+ Finish Installation and restart\n+ If your PC automatically use Windows, go to BIOS Setting again and choose ubuntu to start\n\n## Problems\n+ When you switch your system, the system time will be incorrect:\n\n    ```bash\n    // Solution: Change UTC=yes to UTC=no in Linux \n    // type in cmd:\n    timedatectl set-local-rtc 1 --adjust-system-clock\n    ```\n\n+ Installation freezes at the start and show checking nvidia driver:\n\n    ```txt\n    Solution: \n    1. when you see \"install Ubuntu\" etc.. (GRUB) press \"e\" quickly! \n    2. Edit: Replace \"quiet splash\" to \"nomodeset\" and press F10 to boot.\n    3. After installation done, and reboot.\n    4. At the same place(GRUB), press \"e\" and in the line that starts with \"linux\", add \"nouveau.modeset=0\" at the end of that line. \n    5. When you successfully enter Ubuntu, use Software and Update to install Nvidia Driver.\n    ```\n\n+ Connect Window's Disk in Ubuntu:\n    ```txt\n    Solution:\n    1. with `sudo fdisk -l` you could get an overview of all disk partitions.\n    2. mark the name of device used by your windows disk, e.g. /dev/sda2.\n    3. `sudo gedit /etc/fstab`\n    4. add a line at the end with e.g.\n    `/dev/sda2  /media/Data`\n    5. the device would be able to visited through `/media/Data` in your ubuntu system\n    ```\n","tags":["Ubuntu"]},{"title":"CMake VS Code Setup on Windows","url":"/2021/08/13/CMake-VS-Code-Setup-on-Windows/","content":"# CMake VS Code Setup on Windows\nIn this article, I would like to introduce how to use CMake with VS Code on Windows.\n## What's CMake\n----\nCMake(Cross Platform Make) is free and open-source software for build automation, testing, packaging and installation of software by using a compiler-independent method. It's widely used in software projects. CMake offers higher-level mechanisms like external library detection and configuration (i.e. automatically setting up the defines, include directories and link files for working with a 3rd party library), support for generating installation (and packaging), integration with the CTest test utility and so on.\n\n\n## Installation\n----\nOn Windows, CMake could be directly downloaded und installed with their [official website](https://cmake.org/download/).\nAfter Installation, in order to use `cmake` in cmd, you have to add the path to your environments' variables following:\n\n\n    1. Click Start, type Accounts in the Start search box, and then click User Accounts under Programs.\n\n    2. If you are prompted for an administrator password or for a confirmation, type the password, or click Allow.\n\n    3. In the User Accounts dialog box, click Change my environment variables under Tasks.\n\n    4. Make the changes that you want to the user environment variables for your user account, and then click OK.\nTo verify installation, you could type `cmake --version` in your cmd/powershell.\n\nIn VS Code, make sure the `Extensions` named `CMake` and `CMake Tools` successfully installed.\n\n## Setup\n----\nAt the beginning, create a folder and use VS Code to open it. We'll use this folder to test the functions of CMake.  \n\nAdditionally, make sure you've correct configuration for compiler path in `C/C++: Edit Configurations(UI)`(use ctrl+shift+P and type in). In my case, the path is `C:/msys64/mingw64/bin/gcc.exe`.\n\nIt's highly recommanded that a C++ project with CMake use the folder architecture as follows:\n```bash\n|- test_folder\n    |--.vscode\n    |    |-- c_cpp_properties.json\n    |    |-- tasks.json\n    |    └-- launch.json\n    |-- build\n    |-- include\n    |    └-- your-head-files\n    |-- src\n    |    └-- source_code.cpp\n    └-- CMakeLists.txt\n```\nif you don't know how to get `c_cpp_properties.json`, `tasks.json`, `launch.json`, please refer to [here](https://code.visualstudio.com/docs/cpp/config-msvc) and [Debug using launch.json](https://code.visualstudio.com/docs/cpp/launch-json-reference) for help.\n\nThe core part is how to write the `CMakeLists.txt`:\nActually `cmake` is using `CMakeLists.txt` to execute build. A simple example:\n```cmake\ncmake_minimum_required (VERSION 2.8...3.21.1)\n\nproject(project_name)\n\ninclude_directories(${CMAKE_SOURCE_DIR}/include)\n\naux_source_directory(${CMAKE_SOURCE_DIR}/src DIR_SRC)\n\nadd_executable (project_name ${DIR_SRC})\n```\nThe variables and commands for `CMakeLists.txt` will be introduced in next part.\n\nNow we have all requirements for cmake, let's try:\n```bash\ncd build\ncmake ..\nmake\n```\nIf you don't have `make` installed, here is a short cut to make it work:\n1. Go to the folder where you install you compiler MinGW, in my case it's under `C:\\msys64\\mingw64\\bin`.\n2. Find a file named `mingw32-make.exe` and make a copy in the same foler.\n3. Rename the copy to `make.exe`.\n4. Check with `make --version` in cmd or powershell.\n\nIf the codes above work, you'll see *.exe created under `/build`, cmake works successfully.\n\n## Useful Variables and Commands for CMakeLists.txt\n----\nThe meaning of following words are able to be found under this [documentaion](https://cmake.org/cmake/help/latest/index.html)\n+ Variables: \n    ```cmake\n    CMAKE_SOURCE_DIR\n\n    PROJECT_SOURCE_DIR\n\n    CMAKE_CURRENT_SOURCE_DIR\n\n    CMAKE_MODULE_PATH\n\n    EXECUTABLE_OUTPUT_PATH\n\n    LIBRARY_OUTPUT_PATH\n\n    PROJECT_NAME\n    ```\n+ Commands:\n    ```cmake\n    // setup project name and supported progamming language\n    project(projectname [CXX] [C] [Java])\n\n    // setup the supported cmake version \n    CMAKE_MINIMUM_REQUIRED(VERSION versionNumber [FATAL_ERROR])\n\n    // setup variables\n    SET(VAR [VALUE] [CACHE TYPE DOCSTRING [FORCE]])\n    set(EXECUTABLE_OUTPUT_PATH ${PROJECT_BINARY_DIR}/bin)\n\n    // find all source code files and make them into a list\n    aux_source_directory(dir VARIABLE)\n\n    // add subdirectory to the build\n    add_subdirectory(source_dir [binary_dir][EXCLUDE_FROM_ALL])\n\n    // find head files\n    include_directories([AFTER|BEFORE] [SYSTEM] dir1 dir2 ...)\n\n    // Adds an executable target called <name> to be built from the source files listed\n    add_executable(<name> [source1] [source2 ...])\n\n    // ...\n    ```\n## Reference:\n[1] [CMake Wiki](https://en.wikipedia.org/wiki/CMake) \\\n[2] [What are the benefits/purposes of cmake?](https://stackoverflow.com/questions/19686223/what-are-the-benefits-purposes-of-cmake) \\\n[3] [CMake Documentation](https://cmake.org/cmake/help/latest/index.html#)\n\n\n\n","tags":["CMake","VS Code"]},{"title":"How to Create Personal Blog on github Using Hexo","url":"/2021/08/11/personal_blogs/","content":"\n## Dependencies\n+ Node.js\n+ npm\n+ Git\n\n\n## Install Hexo\nif you don't have `node.js` and npm `npm` on your workstation, please visit [official site](https://nodejs.org/en/) to install the node.js. (npm would be automatically installed if node.js installation is done)\nTo check whether node.js and npm are successfully installed:\n```bash\n# in cmd/terminal\nnode -v # v14.17.3 or others\nnpm -v # 6.14.13 or others\n``` \nIf you are using win10 as your operate system and get errors with these codes, you should re-open the powershell/cmd as administor after the installation then try the codes above.\n\nThen:\n```bash\nnpm install -g hexo-cli\n```\nTo check whether hexo successfully installed:\n```bash\nhexo -v\n```\nYou'll see the version information e.g.:\n```bash\n hexo-cli:4.3.0, \n node:14.17.3, \n ... \n```\n\n## Create Blogs\nFirst of all, you have to navigate to the the folder, where you plan to create your blogs' folder.\n```bash\ncd E:\\\nmkdir my_blogs\ncd my_blogs\n```\nThen run:\n```bash\n# if you are using powershell as administor on windows,\nhexo init\n# if you are on mac/linux, \nsudo hexo init\n```\nCreate your first post:\n```bash\nhexo new \"My First Post\"\n```\nA new file with .md format will be created in `source/_posts/`. You could directly edit it in file. Also, you could preview your blog on your local machine with:\n```bash\n#establish a localhost for blog\nhexo server\n\n# if you have some updates to your blogs, use\nhexo clean\nhexo generate\nhexo server\n```\nThe response will show you a addresse like `http://localhost:4000`, you could visit it on local.\n\n\n## Deploy Blogs\nAfter you know how to write and make changes to your blogs on local, we'll introduce how to create a github page and deploy your blog to it. Install Deloyer with:\n```bash\nnpm install -save hexo-deployer-git\n``` \nGo to your github and create a new repositery called `username.github.io`. `username` should be your github username. This will be used as the address of github page later.\n\nOn your local, you need to edit the `_config.yml`. You could find this file in the blog folder. `_config.yml` is the key to set up your blogs, such as set up the name, discription, theme and so on. \n```bash\n# in _config.yml, find the part #Deployment change the info like below\ndeploy:\n  type: 'git'\n  repo: your-repo-adress\n  branch: master\n```\nAfter this, in your command line type:\n```bash\nhexo deploy\n```\nAfter a while, you would be able to visit `username.github.io` to see your own blog.\n\n## Change the Theme of Your Blog\nThere are varous free themes on [Hexo's official site](https://hexo.io/themes/). You could pick one to visit its github repo and follow the instruction to install it.\n\nE.g. I'd like to change the theme of my blog, firstly, \n```bash\n# in the path of your blog folder, clone the theme to local\ngit clone `url_of_the_theme_repo` themes/my_theme\n```\nThen modify the `_config.yml`. (most theme repos have demo, you could directly compare the `_config.yml` to your own and make some changes)\n\nAfter this:\n```bash\nhexo clean\nhexo generate\n# take a preview on local, \nhexo server\n# if everything is fine,\nhexo deploy\n```\n\n\n\nyou've successfully created your own blog! Congrats!","tags":["Hexo","Blog"]}]