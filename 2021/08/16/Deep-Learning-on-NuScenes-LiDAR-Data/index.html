<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="keyword"  content="">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
        Deep Learning on LiDAR Data P1-Dataset - Ken&#39;s TechBlog
        
    </title>

    <!-- Custom CSS -->
    
<link rel="stylesheet" href="/css/aircloud.css">

    
<link rel="stylesheet" href="/css/gitment.css">

    <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script>
<meta name="generator" content="Hexo 5.4.0"></head>

<body>

<div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> write something </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <img src="/imgs/panda.jpeg" />
        </div>
        <div class="name">
            <i>Ken Lin</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/">
                    <i class="iconfont icon-shouye1"></i>
                    <span>HOME</span>
                </a>
            </li>
            <li >
                <a href="/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>TAGS</span>
                </a>
            </li>
            <li >
                <a href="/archives">
                    <i class="iconfont icon-guidang2"></i>
                    <span>ARCHIVES</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>ABOUT</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <i class="iconfont icon-sousuo1"></i>
                    <span>SEARCH</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#P1-NuScenes-Dataset-Introduction"><span class="toc-text">P1 - NuScenes Dataset Introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Why-NuScenes"><span class="toc-text">Why NuScenes?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Toolkit-Devkit-Installation"><span class="toc-text">Toolkit(Devkit) Installation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#What-could-we-do-with-Dev-Kit"><span class="toc-text">What could we do with Dev-Kit?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Data-Format-and-Usage"><span class="toc-text">Data Format and Usage</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reference"><span class="toc-text">Reference</span></a></li></ol></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">search</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>

        <div class="index-about-mobile">
            <i> write something </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        Deep Learning on LiDAR Data P1-Dataset
    </div>

    <div class="post-meta">
        <span class="attr">Post：<span>2021-08-16 09:38:16</span></span>
        
        <span class="attr">Tags：/
        
        <a class="tag" href="/tags/#NuScenes" title="NuScenes">NuScenes</a>
        <span>/</span>
        
        <a class="tag" href="/tags/#Deep Learning" title="Deep Learning">Deep Learning</a>
        <span>/</span>
        
        <a class="tag" href="/tags/#LiDAR" title="LiDAR">LiDAR</a>
        <span>/</span>
        
        
        </span>
        <span class="attr">Visit：<span id="busuanzi_value_page_pv"></span>
</span>
</span>
    </div>
    <div class="post-content no-indent">
        <h1 id="P1-NuScenes-Dataset-Introduction"><a href="#P1-NuScenes-Dataset-Introduction" class="headerlink" title="P1 - NuScenes Dataset Introduction"></a>P1 - NuScenes Dataset Introduction</h1><h2 id="Why-NuScenes"><a href="#Why-NuScenes" class="headerlink" title="Why NuScenes?"></a>Why NuScenes?</h2><p>With increasing inverstment in fields of autonomous driving, tech-companies are creating their own dataset for training their autonomous vehicles. Some of them, like Waymo Dataset, Lyft L5 Dataset and NuScenes Data are widely used in personal uncommercial reasearchs because of their open-source and mutimodal features.</p>
<p>NuScenes Dataset as one of the earliest published dataset in this field contains various kinds of data collected by different sensors such as cameras, radar and LiDARs. It provides toolkit to help researchers easily and quickly get an overview and process the data with provided functions. And Lyft L5 Dataset also uses similiar toolkit.</p>
<p>Thus, let’s start with processing NuScenes Dataset. Since we are focusing on deep learning with LiDAR data, which means we have to prepar the LiDAR data for our deep learning neural network, we’ll first take an overview of the most important factors of dataset:</p>
<ul>
<li>Size</li>
<li>Variety of Scenes</li>
<li>Number and Quality of Annotated Objects</li>
</ul>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Scenes</th>
<th>Size</th>
<th>PCs lidar</th>
<th>Ann.Frames</th>
<th>3D boxes</th>
</tr>
</thead>
<tbody><tr>
<td>NuScenes</td>
<td>1k</td>
<td>5.5hr</td>
<td>400k</td>
<td>40k</td>
<td>1.4M</td>
</tr>
<tr>
<td>Lyft L5</td>
<td>366</td>
<td>2.5hr</td>
<td>46k</td>
<td>46k</td>
<td>1.3M</td>
</tr>
<tr>
<td>Waymo Open</td>
<td>1k</td>
<td>5.5hr</td>
<td>200k</td>
<td>200k</td>
<td>12M</td>
</tr>
</tbody></table>
<p>As the table shown above, NuScenes has a larger data size comparing to Lyft L5, and has an easy-to-use toolkit comparing to Waymo Dataset. For training a neural network, the more data we have, the better performance we could achieve. And with more different driving scenes and more different objects classes, the neural network will become robuster after training.</p>
<h2 id="Toolkit-Devkit-Installation"><a href="#Toolkit-Devkit-Installation" class="headerlink" title="Toolkit(Devkit) Installation"></a>Toolkit(Devkit) Installation</h2><p>Devkit provides us various functions to extract the specific data format and data information from dataset. It also contains different matricies for evaluating NN(Neural Network)’s performance with respect to your goals such as prediction, detecton and so on.</p>
<ul>
<li>visit <a target="_blank" rel="noopener" href="https://www.nuscenes.org/">official website</a> and account registration</li>
<li>Download Full dataset(v1.0): Trainval and Test Set</li>
<li>Devkit Installation (for more information please visit their <a target="_blank" rel="noopener" href="https://github.com/nutonomy/nuscenes-devkit">github</a>)</li>
</ul>
<blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># under Ubuntu or MacOS</span></span><br><span class="line"><span class="comment"># if you do not have python 3.6/3.7</span></span><br><span class="line"><span class="comment"># install python first</span></span><br><span class="line">sudo apt install python-pip</span><br><span class="line">sudo add-apt-repository ppa:deadsnakes/ppa</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install python3.7</span><br><span class="line">sudo apt-get install python3.7-dev</span><br><span class="line"><span class="comment"># then create virtual environment via conda or virtualenvwrapper</span></span><br><span class="line"><span class="comment"># if you do not have miniconda, google and install it</span></span><br><span class="line"><span class="comment"># after miniconda installed</span></span><br><span class="line">conda create --name deep-learning-nuscenes python=3.7</span><br><span class="line">conda activate deep-learning-nuscenes</span><br><span class="line"><span class="comment"># to deactivate env, use `source deactivate`</span></span><br><span class="line"><span class="comment"># then use</span></span><br><span class="line">pip install nuscenes-devkit</span><br></pre></td></tr></table></figure>
</blockquote>
<ul>
<li>Verify the installation and follow the tutorials <a target="_blank" rel="noopener" href="https://www.nuscenes.org/nuscenes?tutorial=nuscenes">here</a>.</li>
</ul>
<h2 id="What-could-we-do-with-Dev-Kit"><a href="#What-could-we-do-with-Dev-Kit" class="headerlink" title="What could we do with Dev-Kit?"></a>What could we do with Dev-Kit?</h2><p>With devkit of nuscenes we could get an overview of the whole dataset and understand the data formats and structure inside the dataset. We would also use the functions, which devkit provides us, in future steps to build a dataset pre-processing pipeline.</p>
<p>As the introduction, let us try some in jupyter notebook to get better understand of the data formats of the dataset.</p>
<p>To receive an overview of the dataset:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nuscenes.nuscenes <span class="keyword">import</span> NuScenes</span><br><span class="line"><span class="comment"># if you download the mini-dataset</span></span><br><span class="line"><span class="comment"># use version=&quot;v1.0-mini&quot;</span></span><br><span class="line"><span class="comment"># dataroot should be the path</span></span><br><span class="line"><span class="comment"># where you store your dataset</span></span><br><span class="line">nusc = NuScenes(version = <span class="string">&quot;v1.0-trainval&quot;</span>, dataroot=<span class="string">&quot;/home/ken/Data/Dataset/NuScenes&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># ======</span></span><br><span class="line"><span class="comment"># Loading NuScenes tables for version v1.0-trainval...</span></span><br><span class="line"><span class="comment"># 23 category,</span></span><br><span class="line"><span class="comment"># 8 attribute,</span></span><br><span class="line"><span class="comment"># 4 visibility,</span></span><br><span class="line"><span class="comment"># 64386 instance,</span></span><br><span class="line"><span class="comment"># 12 sensor,</span></span><br><span class="line"><span class="comment"># 10200 calibrated_sensor,</span></span><br><span class="line"><span class="comment"># 2631083 ego_pose,</span></span><br><span class="line"><span class="comment"># 68 log,</span></span><br><span class="line"><span class="comment"># 850 scene,</span></span><br><span class="line"><span class="comment"># 34149 sample,</span></span><br><span class="line"><span class="comment"># 2631083 sample_data,</span></span><br><span class="line"><span class="comment"># 1166187 sample_annotation,</span></span><br><span class="line"><span class="comment"># 4 map,</span></span><br><span class="line"><span class="comment"># Done loading in 37.807 seconds.</span></span><br><span class="line"><span class="comment"># ======</span></span><br><span class="line"><span class="comment"># Reverse indexing ...</span></span><br><span class="line"><span class="comment"># Done reverse indexing in 6.4 seconds.</span></span><br><span class="line"><span class="comment"># ======</span></span><br></pre></td></tr></table></figure>

<p>As above shown, we could see there are several data types in this dataset, e.g. <code>category</code>, <code>attribute</code>, <code>visibility</code>, <code>instance</code>, <code>sensor</code>, <code>calibrated_sensor</code>, <code>ego_pose</code>, <code>scene</code>, … and so on. You can find the official definition <a target="_blank" rel="noopener" href="https://www.nuscenes.org/nuscenes#data-format">here</a>.</p>
<p>For our focus, <code>scene</code>, <code>sample</code>, <code>sample_data</code>, <code>sample_annotation</code> will be important.</p>
<ol>
<li><p>scene</p>
<p>To get information of scenes:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">nusc.list_scenes()</span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># scene-0161, Car overtaking, parking lot, peds, ped ... [18-05-21 15:07:23]   19s, boston-seaport, anns:1970</span></span><br><span class="line"><span class="comment"># scene-0162, Leaving parking lot, parked cars, hidde... [18-05-21 15:07:43]   19s, boston-seaport, anns:2230</span></span><br><span class="line"><span class="comment"># ...</span></span><br></pre></td></tr></table></figure>

<p>Watching these outputs, we could know the scenes contain different driving scenarios and each scene lasts about 20 seconds.</p>
<p>For a specific scene:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">my_scene = nusc.scene[<span class="number">0</span>]</span><br><span class="line">my_scene</span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># &#123;&#x27;token&#x27;: &#x27;73030fb67d3c46cfb5e590168088ae39&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;log_token&#x27;: &#x27;6b6513e6c8384cec88775cae30b78c0e&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;nbr_samples&#x27;: 40,</span></span><br><span class="line"><span class="comment"># &#x27;first_sample_token&#x27;: &#x27;e93e98b63d3b40209056d129dc53ceee&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;last_sample_token&#x27;: &#x27;40e413c922184255a94f08d3c10037e0&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;name&#x27;: &#x27;scene-0001&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;description&#x27;: &#x27;Construction, maneuver between several trucks&#x27;&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># explanation:</span></span><br><span class="line"><span class="comment"># &#x27;token&#x27; -- Code for searching and calling this scene</span></span><br><span class="line"><span class="comment"># &#x27;log_token&#x27; -- Code for searching and calling this scene&#x27;s log</span></span><br><span class="line"><span class="comment"># &#x27;nbr_samples&#x27; -- number of samples in this scene</span></span><br><span class="line"><span class="comment"># &#x27;first_sample_token/last_sample_token&#x27; -- Code for searching and calling this scene&#x27;s first/last frame</span></span><br><span class="line"><span class="comment"># &#x27;name&#x27;, &#x27;decription&#x27; -- other information</span></span><br></pre></td></tr></table></figure></li>
<li><p>sample</p>
<p>A sample means a frame. In this dataset, the data is collected every 0.1 second(10 Hz), and it is annotated every half a second(2 Hz), which means every 5 frames, we would have 1 annotated sample. In annotated sample, the existed objects will be annotated.</p>
<p>Let us take a sample as example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">first_sample_token = my_scene[<span class="string">&#x27;first_sample_token&#x27;</span>]</span><br><span class="line">my_sample = nusc.get(<span class="string">&#x27;sample&#x27;</span>, first_sample_token)</span><br><span class="line">my_sample</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># &#123;&#x27;token&#x27;: &#x27;e93e98b63d3b40209056d129dc53ceee&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;timestamp&#x27;: 1531883530449377,</span></span><br><span class="line"><span class="comment"># &#x27;prev&#x27;: &#x27;&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;next&#x27;: &#x27;14d5adfe50bb4445bc3aa5fe607691a8&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;scene_token&#x27;: &#x27;73030fb67d3c46cfb5e590168088ae39&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;data&#x27;: &#123;&#x27;RADAR_FRONT&#x27;: &#x27;bddd80ae33ec4e32b27fdb3c1160a30e&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;RADAR_FRONT_LEFT&#x27;: &#x27;1a08aec0958e42ebb37d26612a2cfc57&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;RADAR_FRONT_RIGHT&#x27;: &#x27;282fa8d7a3f34b68b56fb1e22e697668&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;RADAR_BACK_LEFT&#x27;: &#x27;05fc4678025246f3adf8e9b8a0a0b13b&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;RADAR_BACK_RIGHT&#x27;: &#x27;31b8099fb1c44c6381c3c71b335750bb&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;LIDAR_TOP&#x27;: &#x27;3388933b59444c5db71fade0bbfef470&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;CAM_FRONT&#x27;: &#x27;020d7b4f858147558106c504f7f31bef&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;CAM_FRONT_RIGHT&#x27;: &#x27;16d39ff22a8545b0a4ee3236a0fe1c20&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;CAM_BACK_RIGHT&#x27;: &#x27;ec7096278e484c9ebe6894a2ad5682e9&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;CAM_BACK&#x27;: &#x27;aab35aeccbda42de82b2ff5c278a0d48&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;CAM_BACK_LEFT&#x27;: &#x27;86e6806d626b4711a6d0f5015b090116&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;CAM_FRONT_LEFT&#x27;: &#x27;24332e9c554a406f880430f17771b608&#x27;&#125;,</span></span><br><span class="line"><span class="comment"># &#x27;anns&#x27;: [&#x27;173a50411564442ab195e132472fde71&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;5123ed5e450948ac8dc381772f2ae29a&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;acce0b7220754600b700257a1de1573d&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;8d7cb5e96cae48c39ef4f9f75182013a&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;f64bfd3d4ddf46d7a366624605cb7e91&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;f9dba7f32ed34ee8adc92096af767868&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;086e3f37a44e459987cde7a3ca273b5b&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;3964235c58a745df8589b6a626c29985&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;31a96b9503204a8688da75abcd4b56b2&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;b0284e14d17a444a8d0071bd1f03a0a2&#x27;]&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># explanation:</span></span><br><span class="line"><span class="comment"># &#x27;token&#x27; -- Code for searching and calling this sample</span></span><br><span class="line"><span class="comment"># &#x27;timestamp&#x27; -- the timestamp of this frame</span></span><br><span class="line"><span class="comment"># &#x27;prev&#x27;/&#x27;next&#x27; -- the frame of previous/next frame(before/after 0.1 sec).</span></span><br><span class="line"><span class="comment"># `scene` -- the token of scene, which this sample belongs to.</span></span><br><span class="line"><span class="comment"># `data` -- the token of data, which different sensors collect at this time frame, we could see different radar, lidar and cameras</span></span><br><span class="line"><span class="comment"># `anns` -- the token of annotated objects in this frame</span></span><br></pre></td></tr></table></figure></li>
<li><p>sample_data</p>
<p>sample data means the data collected by a specific sensor in a frame. In part of sample, we could know the dataset contains data collected by RADAR, LiDAR, and Cameras. Let us take look how lidar data looks like:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">sensor = <span class="string">&#x27;LIDAR_TOP&#x27;</span></span><br><span class="line">lidar_data = nusc.get(<span class="string">&#x27;sample_data&#x27;</span>, my_sample[<span class="string">&#x27;data&#x27;</span>][sensor])</span><br><span class="line">lidar_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># &#123;&#x27;token&#x27;: &#x27;3388933b59444c5db71fade0bbfef470&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;sample_token&#x27;: &#x27;e93e98b63d3b40209056d129dc53ceee&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;ego_pose_token&#x27;: &#x27;3388933b59444c5db71fade0bbfef470&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;calibrated_sensor_token&#x27;: &#x27;7a0cd258d096410eb68251b4b87febf5&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;timestamp&#x27;: 1531883530449377,</span></span><br><span class="line"><span class="comment"># &#x27;fileformat&#x27;: &#x27;pcd&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;is_key_frame&#x27;: True,</span></span><br><span class="line"><span class="comment"># &#x27;height&#x27;: 0,</span></span><br><span class="line"><span class="comment"># &#x27;width&#x27;: 0,</span></span><br><span class="line"><span class="comment"># &#x27;filename&#x27;: &#x27;samples/LIDAR_TOP/n015-2018-07-18-11-07-57+0800__LIDAR_TOP__1531883530449377.pcd.bin&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;prev&#x27;: &#x27;&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;next&#x27;: &#x27;bc2cd87d110747cd9849e2b8578b7877&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;sensor_modality&#x27;: &#x27;lidar&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;channel&#x27;: &#x27;LIDAR_TOP&#x27;&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># explanation:</span></span><br><span class="line"><span class="comment"># a set of &#x27;token&#x27; -- code for corresponding object</span></span><br><span class="line"><span class="comment"># &#x27;timestamp&#x27; -- the timestamp of collection</span></span><br><span class="line"><span class="comment"># &#x27;fileformat&#x27; -- here the lidar data is in point cloud format</span></span><br><span class="line"><span class="comment"># &#x27;is_key_frame&#x27; -- keyframe means the frame is annotated</span></span><br><span class="line"><span class="comment"># &#x27;heigt&#x27;/&#x27;width&#x27; -- 3D lidar data does not have these attributes</span></span><br><span class="line"><span class="comment"># &#x27;filename&#x27; -- path of the stored data</span></span><br><span class="line"><span class="comment"># &#x27;prev&#x27;/&#x27;next&#x27; -- token of data in previous/next frame</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>For visualization of the lidar data:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nusc.render_sample_data(lidar_data[<span class="string">&#x27;token&#x27;</span>])</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/adskenlin/adskenlin.github.io/master/2021/08/16/Deep-Learning-on-NuScenes-LiDAR-Data/lidar_vis.png"></p>
</li>
<li><p>sample_annotation</p>
<p>sample_annotation refers to the anntated information of an object in a sample.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">my_annotation_token = my_sample[<span class="string">&#x27;anns&#x27;</span>][<span class="number">5</span>]</span><br><span class="line">my_annotation_metadata =  nusc.get(<span class="string">&#x27;sample_annotation&#x27;</span>, my_annotation_token)</span><br><span class="line">my_annotation_metadata</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># &#123;&#x27;token&#x27;: &#x27;f9dba7f32ed34ee8adc92096af767868&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;sample_token&#x27;: &#x27;e93e98b63d3b40209056d129dc53ceee&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;instance_token&#x27;: &#x27;076e76a589dd4f40adce27b3f3377f58&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;visibility_token&#x27;: &#x27;4&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;attribute_tokens&#x27;: [&#x27;cb5118da1ab342aa947717dc53544259&#x27;],</span></span><br><span class="line"><span class="comment"># &#x27;translation&#x27;: [1009.009, 598.528, 0.664],</span></span><br><span class="line"><span class="comment"># &#x27;size&#x27;: [1.871, 4.478, 1.456],</span></span><br><span class="line"><span class="comment"># &#x27;rotation&#x27;: [0.8844059033120215, 0.0, 0.0, 0.46671854279302766],</span></span><br><span class="line"><span class="comment"># &#x27;prev&#x27;: &#x27;&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;next&#x27;: &#x27;21ece7170dfa431bb504e15f68fc40ce&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;num_lidar_pts&#x27;: 151,</span></span><br><span class="line"><span class="comment"># &#x27;num_radar_pts&#x27;: 3,</span></span><br><span class="line"><span class="comment"># &#x27;category_name&#x27;: &#x27;vehicle.car&#x27;&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># explanation:</span></span><br><span class="line"><span class="comment"># a set of token -- code for corresponding object</span></span><br><span class="line"><span class="comment"># &#x27;translation&#x27; -- the global coordinate system of center of this object</span></span><br><span class="line"><span class="comment"># &#x27;size&#x27; -- the size(width, length, height) of this object</span></span><br><span class="line"><span class="comment"># &#x27;rotation&#x27; -- the orientation of the object shown in quaternion</span></span><br><span class="line"><span class="comment"># &#x27;num_lidar_pts&#x27; -- number of lidar points on this object</span></span><br><span class="line"><span class="comment"># &#x27;category_name&#x27; -- object class name </span></span><br></pre></td></tr></table></figure>

<p>For visulization of this object:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nusc.render_annotation(my_annotation_token)</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/adskenlin/adskenlin.github.io/master/2021/08/16/Deep-Learning-on-NuScenes-LiDAR-Data/lidar_vis.png"></p>
</li>
</ol>
<h2 id="Data-Format-and-Usage"><a href="#Data-Format-and-Usage" class="headerlink" title="Data Format and Usage"></a>Data Format and Usage</h2><p>As above shown, we’ve introduced different data formats in NuScenes dataset. The data formats and their corresponding functions could help us build data processing pipeline in future steps.</p>
<p>Before we build the data pre-processing pipeline, we need to think about: What kind of LiDAR data do we need for deep learning? An example of senor data processing from SOTA methods like Center-Net, MultiXNet shows the pipeline as follows: </p>
<p><img src="https://raw.githubusercontent.com/adskenlin/adskenlin.github.io/master/2021/08/16/Deep-Learning-on-NuScenes-LiDAR-Data/lidardata_pipeline.PNG"></p>
<p>We’ll discuss data pre-processing methods and options in next part.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1903.11027.pdf">nuScenes: A multimodal dataset for autonomous driving</a></p>
<p>[2] <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2006.11275.pdf">Center-based 3D Object Detection and Tracking</a></p>

        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>

        <div id="lv-container">
        </div>

    </div>
</div>

    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        

        

        

        
        <li>
            <a target="_blank"  href="https://github.com/adskenlin">
                            <span class="fa-stack fa-lg">
                                <i class="iconfont icon-github"></i>
                            </span>
            </a>
        </li>
        

        
        <li>
            <a target="_blank"  href="https://www.linkedin.com/in/kenlin93">
                            <span class="fa-stack fa-lg">
                                <i class="iconfont icon-linkedin"></i>
                            </span>
            </a>
        </li>
        

    </ul>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        Created By <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>  Theme <a target="_blank" rel="noopener" href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a></p>
</footer>




</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>

<script src="/js/index.js"></script>

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




</html>
