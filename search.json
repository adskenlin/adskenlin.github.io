[{"title":"Tech. Details of ONNX model and ONNX Runtime","url":"/2021/10/06/Tech-Details-of-ONNX-Model-and-ONNX-Runtime/","content":"## Introduction\n\nONNX(Open Neural Network Exchange) is a format of ML models which allows you to transform models between different frameworks. It provides a definition of an extensible computation graph model, as well as definitions of built-in operators and standard data types.\n\nONNX Runtime(ORT) is a performance-focused engine for ONNX models, which inferences efficiently across multiple platforms and hardware.\n\n## Usage\n\n### Installation\n\nONNX and ORT could be installed easily via:\n\n> `pip install onnx`\n\n> `pip install onnxruntime`\n\n> or `pip install onnxruntime-gpu`\n\nFor conversion tools:\n\n> PyTorch: `pip install torch`\n\n> Tensorflow: `pip install tf2onnx`\n\n> sklearn: `pip install skl2onnx`\n\nORT-Training:\n\n> `pip install torch-ort`\n\n### To obtain an ONNX model\n\n+ [ONNX Model Zoo](https://github.com/onnx/models)\n\n  In model zoo, it collects pre-trained models in various fields in ONNX format. For example, it provides Object Detection & Image Segmentation models such as Yolov3, RetinaNet etc.\n\n+ Export from other ML training frameworks\n  \n  e.g. for PyTorch model, PyTorch provides built-in API to convert PyTorch model to ONNX model. \n\n+ Convert using ONNX provided tools\n  \n  e.g. For TensorFlow model, [ONNX' repo](https://github.com/onnx/tensorflow-onnx) provides tools for conversion of TF/Keras/tensorflow.js/TFLite models.\n\n### Code Example\n\n1. PyTorch-Conversion\n    \n   + Export Model\n     ```python\n     torch.onnx.export(model,\n                       torch.randn(1, 28, 28).to(device),\n                       \"mnist_model.onnx\",\n                       input_names = ['input'],\n                       output_names = ['output'])\n     ```\n   \n   + Load Model\n     ```python\n     import onnx\n     onnx_model = onnx.load(\"mnist_model.onnx\")\n     onnx.checker.check_model(onnx_model)\n     ```\n   \n   + Create inference session using `ort.InferenceSession`\n     ```python\n     import onnxruntime as ort\n     import numpy as np\n     x, y = test_data[0][0], test_data[0][1]\n     ort_sess = ort.InferenceSession('mnist_model.onnx')\n     outputs = ort_sess.run(None, {'input': x.numpy()})\n\n     # Print Result \n     predicted, actual = classes[outputs[0][0].argmax(0)], classes[y]\n     print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n     ```\n\n2. TensorFlow-Conversion\n\n   + Save Model\n     ```python\n     import os\n     import tensorflow as tf\n     from tensorflow.keras.applications.resnet50 import ResNet50\n     import onnxruntime\n\n     model = ResNet50(weights='imagenet')\n\n     preds = model.predict(x)\n     print('Keras Predicted:', decode_predictions(preds, top=3)[0])\n     model.save(os.path.join(\"/tmp\", model.name))\n     ```\n   \n   + Conversion and Export\n     ```python\n     import tf2onnx\n     import onnxruntime as rt\n\n     spec = (tf.TensorSpec((None, 224, 224, 3), tf.float32, name=\"input\"),)\n     output_path = model.name + \".onnx\"\n\n     model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13, output_path=output_path)\n     output_names = [n.name for n in model_proto.graph.output]\n     ```\n\n    + Create inference session with `rt.infernnce`\n      ```python\n      providers = ['CPUExecutionProvider']\n      m = rt.InferenceSession(output_path, providers=providers)\n      onnx_pred = m.run(output_names, {\"input\": x})\n\n      print('ONNX Predicted:', decode_predictions(onnx_pred[0], top=3)[0])\n\n      ```\n\n3. ORT-Training Example\n   ```python\n   import torch\n   import torch_ort\n\n   class TestNet(torch.nn.Module):\n       def __init__(self, input_size, hidden_size, num_classes):\n           pass\n       def forward(self, x):\n           return x\n   \n   model = TestNet(input_size=512, hidden_size=256, num_classes=10)\n   # use ORT-Training\n   model = torch_ort.ORTModule(model)\n\n   criterion = torch.nn.CrossEntropyLoss()\n   optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n\n   for data, target in data_loader:\n       optimizer.zero_grad()\n       y_pred = model(data)\n       loss = criterion(output, target)\n\n       loss.backward()\n       optimizer.step()\n   ```\n   As some blogs like [Accelerate PyTorch training with torch-ort](https://cloudblogs.microsoft.com/opensource/2021/07/13/accelerate-pytorch-training-with-torch-ort/), [Accelerate PyTorch transformer model training with ONNX Runtime â€“ a deep dive](https://techcommunity.microsoft.com/t5/azure-ai/accelerate-pytorch-transformer-model-training-with-onnx-runtime/ba-p/2540471) shown, ONNX Runtime could accelerate some deep learning models' fune-tuning by 20%-40% performance in compare with pure PyTorch."},{"title":"Awesome Paper Collection","url":"/2021/10/05/Awesome-Paper-collection/","content":"I did my research basing on a lot of paper, but I never mark down the key ideas of these awesome papers. In this article, I decide to make a list of these SOTA methods. Most of these methods are focusing on object detection tasks, especially in fields of LiDAR Perception. As next step, I'll try to explain the ideas behind them. \n\n\n## Fields: Deep Learning/ Machine Learning Methodology\n\n- [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)\n- [Mask R-CNN](https://arxiv.org/abs/1703.06870)\n- [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497)\n- [YOLOv3: An Incremental Improvement](https://arxiv.org/abs/1804.02767)\n- [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002)\n- [Objects as Points](https://arxiv.org/abs/1904.07850)\n- [Cornernet: Detecting objects as paired keypoints](https://arxiv.org/abs/1808.01244)\n- [DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution](https://arxiv.org/abs/2006.02334)\n- [Cascade R-CNN: High Quality Object Detection and Instance Segmentation](https://arxiv.org/abs/1906.09756)\n- [Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution](https://arxiv.org/abs/1909.06720)\n- [Deformable Convolutional Networks](https://arxiv.org/abs/1703.06211)\n- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n- [Feature Pyramid Networks for Object Detection](https://arxiv.org/abs/1612.03144)\n\n## Fileds: 3D LiDAR Data Deep Learning\n\n- [PointNet++ deep hierarchical feature learning on point sets in a metric space](https://arxiv.org/abs/1706.02413)\n- [VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection](https://arxiv.org/abs/1711.06396)\n- [Pointpillars: Fast encoders for object detection from point clouds](https://arxiv.org/abs/1812.05784)\n- [FreeAnchor: Learning to Match Anchors for Visual Object Detection](https://arxiv.org/abs/1909.02466)\n- [Second: Sparsely embedded convolutional detection](https://www.mdpi.com/1424-8220/18/10/3337)\n- [Center-based 3D Object Detection and Tracking](https://arxiv.org/abs/2006.11275)\n- [Class-balanced Grouping and Sampling for Point Cloud 3D Object Detection](https://arxiv.org/abs/1908.09492)\n- [End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds](https://arxiv.org/abs/1910.06528)\n- [3DSSD: Point-based 3D Single Stage Object Detector](https://arxiv.org/abs/2002.10187)\n- [Deep Hough Voting for 3D Object Detection in Point Clouds](https://arxiv.org/abs/1904.09664)\n- [SSN: Shape Signature Networks for Multi-class Object Detection from Point Clouds](https://arxiv.org/abs/2004.02774)\n- [Designing Network Design Spaces](https://arxiv.org/abs/2003.13678)\n- [From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network](https://arxiv.org/abs/1907.03670)\n- [MVX-Net: Multimodal voxelnet for 3D object detection](https://arxiv.org/abs/1904.01649)\n- [H3DNet: 3D Object Detection Using Hybrid Geometric Primitives](https://arxiv.org/abs/2006.05682)\n- [ImVoxelNet: Image to Voxels Projection for Monocular and Multi-View General-Purpose 3D Object Detection](https://arxiv.org/abs/2106.01178)\n- [ImVoteNet: Boosting 3D Object Detection in Point Clouds with Image Votes](https://arxiv.org/abs/2001.10692)\n- [FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection](https://arxiv.org/abs/2104.10956)\n- [Fast and Furious: Real Time End-to-End 3D Detection, Tracking and Motion Forecasting with a Single Convolutional Net](https://eng.uber.com/research/fast-and-furious-real-time-end-to-end-3d-detection-tracking-and-motion-forecasting-with-a-single-convolutional-net/)\n- [IntentNet: Learning to Predict Intention from Raw Sensor Data](https://eng.uber.com/research/intentnet-learning-to-predict-intention-from-raw-sensor-data/)\n- [MultiXNet: Multiclass Multistage Multimodal Motion Prediction](https://arxiv.org/abs/2006.02000)","tags":["Deep Learning"]},{"title":"Accelerate Deep Learning Model Inference with TensorRT","url":"/2021/10/03/Accelerate-Deep-Learning-Model-with-TensorRT/","content":"## Introduction\n\nNVIDIA has built a set of tools for deep learning process, such as:\n+ NVIDIA Apex for mix precision and distributed training\n+ NVIDIA DALI for optimized data pre-processing\n+ NVIDIA TensorRT for high-performance inference\n\nIn this article, we'll introduce the TensorRT for inference. TensorRT is an SDK developed by NVIDIA that facilitates high performance machine learning inference. It optimises and accelerates the deep learning model especially on NVIDIA devices via:\n\n1. Reduce Mixed Precision: Maximizes throughput by quantizing models to INT8 while preserving accuracy\n2. Layer and Tensor Fusion: Optimizes use of GPU memory and bandwidth by fusing nodes in a kernel\n3. Kernel Auto-Tuning: Selects best data layers and algorithms based on the target GPU platform\n4. Dynamic Tensor Memory: Minimizes memory footprint and reuses memory for tensors efficiently\n5. Multi-Stream Execution: Uses a scalable design to process multiple input streams in parallel\n6. Time Fusion: Optimizes recurrent neural networks over time steps with dynamically generated kernels\n\n**Devices** with TensorRT available:\n\n+ Server class graphic cards such as A100, T4, V100 etc.\n+ Embedded Device such as AGX Xavier, TX2, Nano etc.\n+ PC class graphic cards such as 30-series, 20-series, 10-series, which depend on their compute capability.\n\n**Performance**:\n\nThe optimization depends on the type and size of model, and the type of graphic cards. GPU calculation performs well in parallel and dense processes, especially for accelerating models with a large number of convolution layers and deconvolution layers, while it does not significantly optimise when there are lots of operations (reshape, gather, split etc.).\n\n## Installation\n\n+ Account registration at [nvidia](https://developer.nvidia.com/)\n\n+ Download [TensorRT](https://developer.nvidia.com/tensorrt):\n\n  NVIDIA TensorRT 8.x Download: \n\n  > TensorRT 8.2 EA\n  >\n  > TensorRT 8.0 GA\n  >\n  > TensorRT 8.0 EA\n  >\n  > EA means early access, GA means general availability(stable version). It's recommanded to download latest stable version for normal use.\n\n  For TensorRT 8.0 GA, choose your version w.r.t. your CUDA version and OS. For example, `TensorRT 8.0.1 GA for Ubuntu 20.04 and CUDA 11.3 DEB local repo package`.\n\n+ Install TensorRT:\n\n  ```bash\n  # install\n  os=\"ubuntuxx04\"\n  tag=\"cudax.x-trt8.x.x.x-yyyymmdd\"\n  sudo dpkg -i nv-tensorrt-repo-${os}-${tag}_1-1_amd64.deb\n  sudo apt-key add /var/nv-tensorrt-repo-${os}-${tag}/7fa2af80.pub\n  \n  sudo apt-get update\n  sudo apt-get install tensorrt\n  \n  # if using Python 3.x, install python3-libnvinfer\n  sudo apt-get install python3-libnvinfer-dev\n  # if use TensorRT with TensorFlow, install graphsurgeon-tf\n  sudo apt-get install uff-converter-tf\n  # install ONNX graphsurgeon package if need\n  sudo apt-get install onnx-graphsurgeon\n  ```\n\n+ Verify the installation\n\n  ```bash\n  dpkg -l | grep TensorRT\n  # output example:\n  ii  graphsurgeon-tf                                             8.0.1-1+cuda11.3                      amd64        GraphSurgeon for TensorRT package\n  ii  libnvinfer-bin                                              8.0.1-1+cuda11.3                      amd64        TensorRT binaries\n  ii  libnvinfer-dev                                              8.0.1-1+cuda11.3                      amd64        TensorRT development libraries and headers\n  ii  libnvinfer-doc                                              8.0.1-1+cuda11.3                      all          TensorRT documentation\n  ii  libnvinfer-plugin-dev                                       8.0.1-1+cuda11.3                      amd64        TensorRT plugin libraries\n  ii  libnvinfer-plugin8                                          8.0.1-1+cuda11.3                      amd64        TensorRT plugin libraries\n  ii  libnvinfer-samples                                          8.0.1-1+cuda11.3                      all          TensorRT samples\n  ii  libnvinfer8                                                 8.0.1-1+cuda11.3                      amd64        TensorRT runtime libraries\n  ii  libnvonnxparsers-dev                                        8.0.1-1+cuda11.3                      amd64        TensorRT ONNX libraries\n  ii  libnvonnxparsers8                                           8.0.1-1+cuda11.3                      amd64        TensorRT ONNX libraries\n  ii  libnvparsers-dev                                            8.0.1-1+cuda11.3                      amd64        TensorRT parsers libraries\n  ii  libnvparsers8                                               8.0.1-1+cuda11.3                      amd64        TensorRT parsers libraries\n  ii  onnx-graphsurgeon                                           8.0.1-1+cuda11.3                      amd64        ONNX GraphSurgeon for TensorRT package\n  ii  python3-libnvinfer                                          8.0.1-1+cuda11.3                      amd64        Python 3 bindings for TensorRT\n  ii  python3-libnvinfer-dev                                      8.0.1-1+cuda11.3                      amd64        Python 3 development package for TensorRT\n  ii  tensorrt                                                    8.0.1.6-1+cuda11.3                    amd64        Meta package of TensorRT\n  ii  uff-converter-tf                                            8.0.1-1+cuda11.3                      amd64        UFF converter for TensorRT package\n  \n  ```\n\n+ Install PyCUDA if needed:\n\n  `pip install 'pycuda<2021.1'`\n\n  Some features of PyCUDA include(source: NVIDIA TENSORRT documentation):                              \n\n  - Maps all of CUDA into Python.\n  - Enables run-time code generation (RTCG) for flexible, fast, automatically tuned codes.                              \n  - Added robustness: automatic management of object lifetimes, automatic error checking                              \n  - Added convenience: comes with ready-made on-GPU linear algebra, reduction, scan.                              \n  - Add-on packages for FFT and LAPACK available.\n  - Fast. Near-zero wrapping overhead. \n\n## Usage, Properties, Practices\n\n### a. Official Samples:\n\nThere are lots of code samples in [official documentation](https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html). They contain the fields of Image Classification, Object Detection etc.  As we know, tensorrt has builtin parsers, including caffeparser, uffparser, onnxparser, etc.\n\n1. Python Case Study: [MNIST_Model](https://github.com/NVIDIA/TensorRT/tree/master/samples/python/network_api_pytorch_mnist)\n\n> Transform Type: PyTorch Model ==>> TensorRT Engine\n\n2.  Python Case Study: [EfficientDet Object Detecion](https://github.com/NVIDIA/TensorRT/tree/master/samples/python/efficientdet#build-tensorrt-engine)\n\n> Transform Type: TensorFlow Saved Model(UFF) ==>> ONNX ==>> TensorRT Engine\n\n3. C++ Case Study: [MNIST_Model](https://github.com/NVIDIA/TensorRT/tree/master/samples/sampleOnnxMNIST)\n\n> Transform Type:  ONNX ==>> TensorRT Engine\n\n4. C++ Case Study: [Object Detection with Faster R-CNN](https://github.com/NVIDIA/TensorRT/tree/master/samples/sampleFasterRCNN), [SSD](https://github.com/NVIDIA/TensorRT/tree/master/samples/sampleSSD)\n\n> Transform Type: Caffe Model ==>> TensorRT Engine\n\n### b. Third Party: [Torch2trt](https://github.com/NVIDIA-AI-IOT/torch2trt)\n\nA special repo [torch2trt](https://github.com/NVIDIA-AI-IOT/torch2trt) introduced a PyTorch to TensorRT converter with Python API. With this we could transform some light networks and deploy them on device like Jetson. The basic idea of this repo is to convert the basical operations such as pool, ReLU. It makes model building much more flexible, and help us design our own model. But in many cases, Python is not best case for deploying and PyTorch model is no more flexible across platforms and systems.\n\n```python\n# Conversion\nimport torch\nfrom torch2trt import torch2trt\nfrom torchvision.models.alexnet import alexnet\n\n# create some regular pytorch model...\nmodel = alexnet(pretrained=True).eval().cuda()\n\n# create example data\nx = torch.ones((1, 3, 224, 224)).cuda()\n\n# convert to TensorRT feeding sample data as input\nmodel_trt = torch2trt(model, [x])\n\n# Execution\ny = model(x)\ny_trt = model_trt(x)\n\n# check the output against PyTorch\nprint(torch.max(torch.abs(y - y_trt)))\n\n# Save and load\ntorch.save(model_trt.state_dict(), 'alexnet_trt.pth')\n# load with TRT Module\nfrom torch2trt import TRTModule\nmodel_trt = TRTModule()\nmodel_trt.load_state_dict(torch.load('alexnet_trt.pth'))\n\n```\n\n### c. Third Party: [TensorRTx](https://github.com/wang-xinyu/tensorrtx)\n\nTensorRTx is a repo with various hand-written C++ models. The basic idea of this repo is to write the C++ codes for every SOTA model with pre-trained weights stored in files, then use TensorRT API to call them, in order to build the Tensor RT Engine. It could be very easy to use the existing codes but not very suitable for self-designed models.\n\nAn example of YOLO codes for building engine as follows:\n\n```c++\nCudaEngine* createEngine(unsigned int maxBatchSize, IBuilder* builder, IBuilderConfig* config, DataType dt) {\n    INetworkDefinition* network = builder->createNetworkV2(0U);\n\n    // Create input tensor of shape {3, INPUT_H, INPUT_W} with name INPUT_BLOB_NAME\n    ITensor* data = network->addInput(INPUT_BLOB_NAME, dt, Dims3{3, INPUT_H, INPUT_W});\n    assert(data);\n\n    std::map<std::string, Weights> weightMap = loadWeights(\"../yolov3-tiny.wts\");\n    Weights emptywts{DataType::kFLOAT, nullptr, 0};\n\n    auto lr0 = convBnLeaky(network, weightMap, *data, 16, 3, 1, 1, 0);\n    auto pool1 = network->addPoolingNd(*lr0->getOutput(0), PoolingType::kMAX, DimsHW{2, 2});\n    pool1->setStrideNd(DimsHW{2, 2});\n    auto lr2 = convBnLeaky(network, weightMap, *pool1->getOutput(0), 32, 3, 1, 1, 2);\n    auto pool3 = network->addPoolingNd(*lr2->getOutput(0), PoolingType::kMAX, DimsHW{2, 2});\n    pool3->setStrideNd(DimsHW{2, 2});\n    auto lr4 = convBnLeaky(network, weightMap, *pool3->getOutput(0), 64, 3, 1, 1, 4);\n    auto pool5 = network->addPoolingNd(*lr4->getOutput(0), PoolingType::kMAX, DimsHW{2, 2});\n    pool5->setStrideNd(DimsHW{2, 2});\n    auto lr6 = convBnLeaky(network, weightMap, *pool5->getOutput(0), 128, 3, 1, 1, 6);\n    auto pool7 = network->addPoolingNd(*lr6->getOutput(0), PoolingType::kMAX, DimsHW{2, 2});\n    pool7->setStrideNd(DimsHW{2, 2});\n    auto lr8 = convBnLeaky(network, weightMap, *pool7->getOutput(0), 256, 3, 1, 1, 8);\n    auto pool9 = network->addPoolingNd(*lr8->getOutput(0), PoolingType::kMAX, DimsHW{2, 2});\n    pool9->setStrideNd(DimsHW{2, 2});\n    auto lr10 = convBnLeaky(network, weightMap, *pool9->getOutput(0), 512, 3, 1, 1, 10);\n    auto pad11 = network->addPaddingNd(*lr10->getOutput(0), DimsHW{0, 0}, DimsHW{1, 1});\n    auto pool11 = network->addPoolingNd(*pad11->getOutput(0), PoolingType::kMAX, DimsHW{2, 2});\n    pool11->setStrideNd(DimsHW{1, 1});\n    auto lr12 = convBnLeaky(network, weightMap, *pool11->getOutput(0), 1024, 3, 1, 1, 12);\n    auto lr13 = convBnLeaky(network, weightMap, *lr12->getOutput(0), 256, 1, 1, 0, 13);\n    auto lr14 = convBnLeaky(network, weightMap, *lr13->getOutput(0), 512, 3, 1, 1, 14);\n    IConvolutionLayer* conv15 = network->addConvolutionNd(*lr14->getOutput(0), 3 * (Yolo::CLASS_NUM + 5), DimsHW{1, 1}, weightMap[\"module_list.15.Conv2d.weight\"], weightMap[\"module_list.15.Conv2d.bias\"]);\n    // 16 is yolo\n    auto l17 = lr13;\n    auto lr18 = convBnLeaky(network, weightMap, *l17->getOutput(0), 128, 1, 1, 0, 18);\n\n    float *deval = reinterpret_cast<float*>(malloc(sizeof(float) * 128 * 2 * 2));\n    for (int i = 0; i < 128 * 2 * 2; i++) {\n        deval[i] = 1.0;\n    }\n    Weights deconvwts19{DataType::kFLOAT, deval, 128 * 2 * 2};\n    IDeconvolutionLayer* deconv19 = network->addDeconvolutionNd(*lr18->getOutput(0), 128, DimsHW{2, 2}, deconvwts19, emptywts);\n    assert(deconv19);\n    deconv19->setStrideNd(DimsHW{2, 2});\n    deconv19->setNbGroups(128);\n    weightMap[\"deconv19\"] = deconvwts19;\n\n    ITensor* inputTensors[] = {deconv19->getOutput(0), lr8->getOutput(0)};\n    auto cat20 = network->addConcatenation(inputTensors, 2);\n    auto lr21 = convBnLeaky(network, weightMap, *cat20->getOutput(0), 256, 3, 1, 1, 21);\n    IConvolutionLayer* conv22 = network->addConvolutionNd(*lr21->getOutput(0), 3 * (Yolo::CLASS_NUM + 5), DimsHW{1, 1}, weightMap[\"module_list.22.Conv2d.weight\"], weightMap[\"module_list.22.Conv2d.bias\"]);\n    // 22 is yolo\n\n    auto creator = getPluginRegistry()->getPluginCreator(\"YoloLayer_TRT\", \"1\");\n    const PluginFieldCollection* pluginData = creator->getFieldNames();\n    IPluginV2 *pluginObj = creator->createPlugin(\"yololayer\", pluginData);\n    ITensor* inputTensors_yolo[] = {conv15->getOutput(0), conv22->getOutput(0)};\n    auto yolo = network->addPluginV2(inputTensors_yolo, 2, *pluginObj);\n\n    yolo->getOutput(0)->setName(OUTPUT_BLOB_NAME);\n    network->markOutput(*yolo->getOutput(0));\n\n    // Build engine\n    builder->setMaxBatchSize(maxBatchSize);\n    config->setMaxWorkspaceSize(16 * (1 << 20));  // 16MB\n#ifdef USE_FP16\n    config->setFlag(BuilderFlag::kFP16);\n#endif\n    std::cout << \"Building engine, please wait for a while...\" << std::endl;\n    ICudaEngine* engine = builder->buildEngineWithConfig(*network, *config);\n    std::cout << \"Build engine successfully!\" << std::endl;\n\n    // Don't need the network any more\n    network->destroy();\n\n    // Release host memory\n    for (auto& mem : weightMap)\n    {\n        free((void*) (mem.second.values));\n    }\n\n    return engine;\n}\n```\n\n### d. Third Party: [TensorRT_Pro](https://github.com/shouxieai/tensorRT_Pro)\n\nTensorRT_Pro is one of the repos which are focusing on transforming PyTorch Model to ONNX model, then use ONNX model to build TensorRT Engine. The reasons are, that PyTorch official provides stable methods for transforming PyTorch PT model to ONNX model, and NVIDIA official provides up to now relative better methods for the ONNX-TensorRT-Transfer. With standard ONNX model, it could in some cases solve the problems we could meet under different deploy-environments. It's a very practical repo and I'm looking into it these days.\n\n## Learning Notes\n\n+ Personally, I would perfer to use `PyTorch Model - ONNX model - TRT Engine` route (similar to d. TensorRT_PRO) to accelerate deployment of model on NV devices.\n\n## Reference\n\n1. NVIDIA Official Docs https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#python_example_unsupported\n2.  ONNX supported operators in ONNX-TensorRT: https://github.com/onnx/onnx-tensorrt/blob/master/docs/operators.md","tags":["TensorRT","Deep Learning"]},{"title":"Basic and Advanced Git","url":"/2021/10/01/Basic-and-Advanced-Git/","content":"**Git** is version-control tool for tracking changes in files. It is widely used in software development project and It significantly increases the working efficient among teams and supports for distributed, non-linear workflows (thousands of parallel branches running on different systems). One thing more to mention, it was created by Linus Torvals (Genius!!!).\n\nThis Blog is written for marking down the key points and tricks of git. It will keep updating!\n\n## Installation\n\nFollow the official websites to install git: https://git-scm.com/book/en/v2/Getting-Started-Installing-Git\n\nTo verify the installation, use `git --version` in terminal, you should see the output `git version x.xx.x`.\n\n## Setup\n\n```bash\n# Set up git configs\n\n# 1. Identity\ngit config --global user.name \"YOUR NAME\"\ngit config --global user.email \"YOUR EMAIL\"\n# 2. Default Branch Name\ngit config --global init.defaultBranch main\n# All Info\ngit config --list \n```\n\n## Tips and Tricks\n\n1. If you want to create a git repository at local, try:\n\n   ```bash\n    cd YOUR_REPO\n    git init\n   ```\n\n2. If you want to clone a git repo from remote(github, gitlab, etc.), try:\n\n   ```bash\n   git clone YOUR_URL\n   ```\n\n3. The files in Git system could have 4 types: **Untracked**, **Unmodified**, **Modified**, **Staged**. \n\n4. Check the status of all files:\n\n   ```bash\n   git status\n   # output:\n   On branch master\n   Your branch is up-to-date with 'origin/master'.\n   Changes to be committed:\n     (use \"git reset HEAD <file>...\" to unstage)\n   \n       new file:   README\n       modified:   CONTRIBUTING.md\n   \n   Changes not staged for commit:\n     (use \"git add <file>...\" to update what will be committed)\n     (use \"git checkout -- <file>...\" to discard changes in working directory)\n   \n       modified:   CONTRIBUTING.md\n   ```\n\n   As above shown, `git status` will output the current branch, files to be commited, files to add on stage, etc.\n\n5. If you have some files that you do not want Git to add or track, use file named `.gitignore`, add the name of files.\n\n6. View Changes:\n\n   ```bash\n   git diff\n   git diff --staged\n   ```\n\n7. Commit:\n\n   ```bash\n   # open edit and commit\n   git commit\n   \n   # one-line commit\n   git commit -m \"YOUR COMMIT\"\n   \n   # skip add, one-line commit\n   git commit -a -m \"YOUR COMMIT\"\n   ```\n\n8. Remove files: `git rm FILE`\n\n9. Rename files (actually use move operation):\n\n   ```bash\n   git mv old_file new_file\n   ```\n\n10. View commit history:\n\n    ```bash\n    # normal log\n    git log\n    # limit the log content to n(2) lines\n    git log -p -2\n    # limit the log content w.r.t. time\n    git log --since=2.weeks\n    # show statistc of log\n    git log --stat\n    # specific formats for log\n    git log --pretty=oneline\n    git log --pretty=format:\"%h %s\" --graph\n    # Very specific\n    git log --pretty=\"%h - %s\" --author='Junio C Hamano' --since=\"2008-10-01\" \\\n       --before=\"2008-11-01\" --no-merges -- t/\n    ```\n\n11. Undoing Things:\n\n    ```bash\n    # when you commit too early,\n    # still want to add some files then commit\n    git commit --amend\n    ```\n\n12. Unstaging a staged file:\n\n    ```bash\n    git status\n    # output:\n    On branch master\n    Changes to be committed:\n      (use \"git reset HEAD <file>...\" to unstage)\n    \n        renamed:    README.md -> README\n        modified:   CONTRIBUTING.md\n    git reset HEAD ...\n    ```\n\n13. Unmodifying a modified file:\n\n    ```bash\n    git status\n    # output:\n    Changes not staged for commit:\n      (use \"git add <file>...\" to update what will be committed)\n      (use \"git checkout -- <file>...\" to discard changes in working directory)\n    \n        modified:   CONTRIBUTING.md\n    git checkout -- xxx.md\n    ```\n\n14. `git restore`\n\n    ```bash\n    # unstage a staged file\n    git restore --staged <file>\n    # unmodifying a modified file\n    git restore <file>\n    ```\n\n15. Working with Remotes:\n\n    ```bash\n    # show remotes\n    git remote -v\n    # add remotes\n    git remote add <shortname> <url>\n    # Fetching and Pulling\n    git fetch <remote>\n    git push <remote> <branch>\n    # show remote information\n    git remote show origin\n    # rename and remove remotes\n    git remote rename <name1> <name2>\n    git remote rm <name>\n    ```\n\n16. Working with Tags:\n\n    ```bash\n    # list tags\n    git tag\n    git tag -l \"version-*\"\n    # create annotated tags\n    git tag -a version -m \"commit for version\"\n    # create lightweight tags\n    git tag version\n    # show tag\n    git show tag_name\n    # add tag to an old version\n    git tag -a version LOG_NUM\n    # git push will not transfer tags to remote servers by default\n    git push origin <tagname>\n    git push origin --tags\n    # delete tag\n    git tag -d <tagname>\n    # check out tags\n    git checkout <tagname>\n    # be careful with:\n    git checkout -b <branch> <tagname>\n    ```\n\n17. Aliases:\n\n    ```bash\n    git config --global alias.last 'log -1 HEAD'\n    git last\n    ```\n\n18. BASIC Branches:\n\n    ```bash\n    # create branch (but will not switch to it)\n    git branch <BranchName>\n    # show the branch pointers info\n    git log --oneline --decorate\n    # switch branches\n    git checkout <BranchName>\n    # After some comments, the HEAD will move forward\n    # move HEAD back to master\n    git checkout master\n    # show divergent history\n    git log --oneline --decorate --graph --all\n    \n    \n    # create a new branch and switch\n    git checkout -b <BranchName>\n    # merge a branch, master will fast-forward to BranchName\n    git checkout master\n    git merge <BranchName>\n    # then delete the BranchName because no more needed\n    git branch -d <BranchName>\n    \n    \n    # merge conflicts\n    git merge <BranchName>\n    # output:\n    Auto-merging index.html\n    CONFLICT (content): Merge conflict in index.html\n    Automatic merge failed; fix conflicts and then commit the result.\n    git status\n    # ..., resolve the conflicts\n    # another graphical tool: git mergetool?\n    ```\n\n19. Branches MANAGEMENT:\n\n    ```bash\n    # about git branch\n    # see all branches and last commit on each branch\n    git branch -v\n    # about merged/unmerged branch\n    git branch --merged\n    git branch --no-merged\n    \n    # rename branch\n    git branch --move old-name new-name\n    git push --set-upstream origin new-name\n    # but now the old-name still exists at remote\n    # to delete\n    git push origin --delete old-name\n    \n    \n    \n    ```\n\n20. REMOTE Branches:\n\n    ```bash\n    # REMOTE: R0 <- R1 <- R2 \n    #                     |\n    #                    master\n    #\n    #             origin/master (remote branch)\n    #                    |\n    # LOCAL: R0 <- R1 <- R2\n    #                    |\n    #                   master (local branch)\n    ```\n\n    ```bash\n    # REMOTE: R0 <- R1 <- R2 <- R3 <- R4\n    #                                 |\n    #                               master\n    #\n    #             origin/master (remote branch)\n    #                    |\n    # LOCAL: R0 <- R1 <- R2 <- L3 <- L4\n    #                                |\n    #                              master (local branch)\n    ```\n\n    ```bash\n    # git fetch <remote>\n    git fetch origin\n    ```\n\n    ```bash\n    # After FETCH:\n    #\n    #                        origin/master (remote branch)\n    #                                |\n    # LOCAL: R0 <- R1 <- R2 <- R3 <- R4\n    #                    |\n    #                    <- L3 <- L4\n    #                             |\n    #                        master (local branch)\n    ```\n\n    ```bash\n    # IF THERE IS ANOTHER REMOTE:\n    # REMOTE_1: R0 <- R1 <- R2 <- R3 <- R4\n    #                       |\n    #                     master\n    \n    git fetch REMOTE_1\n    \n    #           REMOTE_1/master     origin/master (REMOTE)\n    #                    |           |\n    # LOCAL: R0 <- R1 <- R2 <- R3 <- R4\n    #                    |\n    #                    <- L3 <- L4\n    #                             |\n    #                        master (local branch)\n    ```\n\n    ```bash\n    # Push:\n    git push <remote> <LocalBranch>:<RemoteBranch>\n    # or if you keep the name of branches\n    git push <remote> <Branch>\n    \n    # Tracking Branches and automatically pull from remote:\n    # create a new one, switch and track\n    git checkout --track origin/branchname\n    # use the local one to track\n    git branch -u origin/branchname\n    \n    # fetch all and show\n    git fetch --all; git branch -vv\n    \n    ```\n\n21. Rebasing:\n\n    ```bash\n    # Integrate changes from one to another\n    # similar to MERGE\n    # if we have two divergent work: master, experiment\n    # switch to experiment\n    git checkout experiment\n    # integrate experiment as the node next to master\n    git rebase master\n    # switch to master\n    git checkout master\n    # merge\n    git merge experiment\n    ```\n\n    ```bash\n    # There are 3 branches: master, server, client\n    \n    git rebase --onto master server client\n    git checkout master\n    git merge client\n    # git rebase <basebranch> <topicbranch>\n    git rebase master server\n    git checkout master\n    git merge server\n    git branch -d client\n    git branch -d server\n    ```\n\n## Reference:\n\n1. 13 Advanced (but useful) Git Technologies and Shortcuts https://www.youtube.com/watch?v=ecK3EnyGD8o\n2. Git Docs https://git-scm.com/book/en/v2\n3. Git Wiki https://en.wikipedia.org/wiki/Git\n\n","tags":["Git"]},{"title":"MMDetection3D: Deep Learning Toolbox for 3D LiDAR Data - P1","url":"/2021/09/07/MMdetection3D-Deep-Learning-Codebase-for-3D-LiDAR-Data/","content":"MMDetection3D is an open-source object detection codebase tool based on PyTorch. It's a project developed by MMLab, which has published several famous toolboxes e.g. MMDetection, MMsegmentation and so on. \n\nFor more information, please visit their [official github repo](https://github.com/open-mmlab/mmdetection3d). In this blog, I would like to introduce this toolbox and explain how it works from its source code.\n\n## Introduction\n\nThis toolbox was built focusing on 3D object detection tasks. If you've heard about `mmdetection` of Openmmlab, you could think this repo as an 3D-extension of basic `mmdetection`. It is built based on `mmdetection` and `mmcv`, and it use nearly same mechanism and coding-style as `mmdetection`.\n\nI would highly recommend this toolbox for 3D Object Detection, since it has everything we need in this field and it works much more efficient than your own hand-written codes. Reasons:\n\n+ It provides functions and built-in dataset classes to process raw 3D Data no matter it's from NuScenes, Lyft or Waymo.\n\n+ It provides a huge number of functions from SOTA methods for processing point cloud data, anchors, bounding boxes, which we may use during our pre-processing step.\n\n+ It contains various operations and their functions, which we may use in different parts of network, such as spare 3D convolution, iou3D calculation.\n\n+ while designing architecture, it provides us various sub-architectures for ablation study. e.g. in different parts of network, such as in `backbone`, `Neck`, `Head`, it provides the architectures from many SOTA methods.\n\n## Installation\n\nFollow the steps [here](https://github.com/open-mmlab/mmdetection3d/blob/master/docs/getting_started.md). One thing you need to pay attention is, that the version of mmcv, python, pytorch and mmdetection should match. I was stucking in a problem for a long time, which causes the last installtion step `pip install -v -e .` failed. Because I have multiple CUDA versions(e.g 10.1, 11.2) installed and it does not match the pytorch version I used.\n\n## Look into Codes\n\nYou could see many folders in `/mmdetection3d`. I would like sort them in 3 levels:\n\n+ Sure to use: `configs`, `mmdet3d`,  `tools`\n\n+ May use: `data`, `tests`\n\n+ not relevant: `others`\n\n### What is Configs?\n\nIn `configs`, you will have to create the config file for your deep learning methods, which should include the information of dataset setup, neural network architecture, training and validation parameters optimization and so on.\n\nThe basic mechanism of this toolbox is, that it has set up all the definitions, parameters in its registry module. When you want to use one of them, you only have to write it in config file and it will be called up automatically. \n\nTo explain it clearly, let's look into an example, Centerpoint: \n\n```python\n\n# read basic config files of different parts\n# for dataset, use nus-3d.py as default\n# for network architecture, use centerpoint_01voxel_second_secfpn_nus.py as default\n# for training params and runtime setup, use cyclic_20e.py and default_runtime.py as default\n_base_ = [\n    '../_base_/datasets/nus-3d.py',\n    '../_base_/models/centerpoint_01voxel_second_secfpn_nus.py',\n    '../_base_/schedules/cyclic_20e.py', '../_base_/default_runtime.py'\n]\n\n# In the following part you have to set up the parameters\n# which should overwrite the default values\n\n# in this part,\n# point_cloud_range and class_names are common params\n# they may be used not only in dataset, \n# and also in models and training loops\npoint_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]\nclass_names = [\n    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',\n    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'\n]\n\n\n# in this part,\n# the params for setting up a model will be partially overwritten\nmodel = dict(\n    pts_voxel_layer=dict(point_cloud_range=point_cloud_range),\n    pts_bbox_head=dict(bbox_coder=dict(pc_range=point_cloud_range[:2])),\n    train_cfg=dict(pts=dict(point_cloud_range=point_cloud_range)),\n    test_cfg=dict(pts=dict(pc_range=point_cloud_range[:2])))\n\n\n# in this part,\n# there are definitions about dataset, data path,\n# database sampler, preprocessing pipeline for training and testing,\n# they are necessary for setting up a dataset object\ndataset_type = 'NuScenesDataset'\ndata_root = 'data/nuscenes/'\nfile_client_args = dict(backend='disk')\n\ndb_sampler = dict(\n    data_root=data_root,\n    info_path=data_root + 'nuscenes_dbinfos_train.pkl',\n    rate=1.0,\n    prepare=dict(\n        filter_by_difficulty=[-1],\n        filter_by_min_points=dict(\n            car=5,\n            truck=5,\n            bus=5,\n            trailer=5,\n            construction_vehicle=5,\n            traffic_cone=5,\n            barrier=5,\n            motorcycle=5,\n            bicycle=5,\n            pedestrian=5)),\n    classes=class_names,\n    sample_groups=dict(\n        car=2,\n        truck=3,\n        construction_vehicle=7,\n        bus=4,\n        trailer=6,\n        barrier=2,\n        motorcycle=6,\n        bicycle=6,\n        pedestrian=2,\n        traffic_cone=2),\n    points_loader=dict(\n        type='LoadPointsFromFile',\n        coord_type='LIDAR',\n        load_dim=5,\n        use_dim=[0, 1, 2, 3, 4],\n        file_client_args=file_client_args))\n\ntrain_pipeline = [\n    dict(\n        type='LoadPointsFromFile',\n        coord_type='LIDAR',\n        load_dim=5,\n        use_dim=5,\n        file_client_args=file_client_args),\n    dict(\n        type='LoadPointsFromMultiSweeps',\n        sweeps_num=9,\n        use_dim=[0, 1, 2, 3, 4],\n        file_client_args=file_client_args,\n        pad_empty_sweeps=True,\n        remove_close=True),\n    dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),\n    dict(type='ObjectSample', db_sampler=db_sampler),\n    dict(\n        type='GlobalRotScaleTrans',\n        rot_range=[-0.3925, 0.3925],\n        scale_ratio_range=[0.95, 1.05],\n        translation_std=[0, 0, 0]),\n    dict(\n        type='RandomFlip3D',\n        sync_2d=False,\n        flip_ratio_bev_horizontal=0.5,\n        flip_ratio_bev_vertical=0.5),\n    dict(type='PointsRangeFilter', point_cloud_range=point_cloud_range),\n    dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),\n    dict(type='ObjectNameFilter', classes=class_names),\n    dict(type='PointShuffle'),\n    dict(type='DefaultFormatBundle3D', class_names=class_names),\n    dict(type='Collect3D', keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])\n]\ntest_pipeline = [\n    dict(\n        type='LoadPointsFromFile',\n        coord_type='LIDAR',\n        load_dim=5,\n        use_dim=5,\n        file_client_args=file_client_args),\n    dict(\n        type='LoadPointsFromMultiSweeps',\n        sweeps_num=9,\n        use_dim=[0, 1, 2, 3, 4],\n        file_client_args=file_client_args,\n        pad_empty_sweeps=True,\n        remove_close=True),\n    dict(\n        type='MultiScaleFlipAug3D',\n        img_scale=(1333, 800),\n        pts_scale_ratio=1,\n        flip=False,\n        transforms=[\n            dict(\n                type='GlobalRotScaleTrans',\n                rot_range=[0, 0],\n                scale_ratio_range=[1., 1.],\n                translation_std=[0, 0, 0]),\n            dict(type='RandomFlip3D'),\n            dict(\n                type='PointsRangeFilter', point_cloud_range=point_cloud_range),\n            dict(\n                type='DefaultFormatBundle3D',\n                class_names=class_names,\n                with_label=False),\n            dict(type='Collect3D', keys=['points'])\n        ])\n]\n\neval_pipeline = [\n    dict(\n        type='LoadPointsFromFile',\n        coord_type='LIDAR',\n        load_dim=5,\n        use_dim=5,\n        file_client_args=file_client_args),\n    dict(\n        type='LoadPointsFromMultiSweeps',\n        sweeps_num=9,\n        use_dim=[0, 1, 2, 3, 4],\n        file_client_args=file_client_args,\n        pad_empty_sweeps=True,\n        remove_close=True),\n    dict(\n        type='DefaultFormatBundle3D',\n        class_names=class_names,\n        with_label=False),\n    dict(type='Collect3D', keys=['points'])\n]\n\ndata = dict(\n    train=dict(\n        type='CBGSDataset',\n        dataset=dict(\n            type=dataset_type,\n            data_root=data_root,\n            ann_file=data_root + 'nuscenes_infos_train.pkl',\n            pipeline=train_pipeline,\n            classes=class_names,\n            test_mode=False,\n            use_valid_flag=True,\n            box_type_3d='LiDAR')),\n    val=dict(pipeline=test_pipeline, classes=class_names),\n    test=dict(pipeline=test_pipeline, classes=class_names))\n\n\n# In this part,\n# some params in runtime or training loop will be overwritten\nevaluation = dict(interval=20, pipeline=eval_pipeline)\n```\n\nTo draw a conclusion, the `configs` is where we define our network. We could use pre-defined definition or built-in functions to build up our neural network. If you think up a parameter or architecture which does not exist in toolbox, you could also registry them in next part `mmdet3d`.\n\n### What is mmdet3d?\n\n`mmdet3d` is the place where you could find and fetch your screwdriver. Under it, there are: `apis`, `core`, `dataset`, `models`, `ops`, `utils`.\n\n+ In `apis` you could find the simple api-codes for training, testing and inference. \n\n+ In `core` you could find many pre-defined tools, which may be helpful in your research. E.g. `Anchor3DRangeGenerator` defines the method object to generate anchors. (for anchor-based methods, anchors could be generated in different ways)  \n\n+ In `dataset` and `models` it provides various dataset/model options for your neural network. E.g. for models, you could choose ResNet as `backbone`, FPN as `neck`, anchor3dhead as `tast_heads`, and combine them as your network architecture.\n\n+ In `ops` it provides many functions for data processing. E.g. Sparse 3D Convolution as an option of processing method for voxelized space.\n\nIf you want to design your own methods or architectures, you could just put the codes in corresponding folders and add your design to registry module. The toolbox could then fetch your self-designed object easily.\n\n### What is tools?\n\nIn `tools`, it provides us codes for creating database, training, testing, parallel training with multi-GPUs or multi-workstations and so on. \n\nBriefly speaking, you'll set up all parameters and arguments of your neural network and training loop in `configs`, if you have some self-designed methods and architectures, you have to put and regitry them in `mmdet3d`. Afterwards you could run training and evaluation process with the codes in `tools`.\n\n## What's coming NEXT\n\nIn my next blog 'MMDetection3D: Deep Learning Codebase for 3D LiDAR Data - P2', we will introduce the details of training in MMDetection3D.","tags":["Deep Learning","LiDAR","Point Cloud Data"]},{"title":"Deep Learning on 3D LiDAR Data P1 Dataset","url":"/2021/08/16/Deep-Learning-on-NuScenes-LiDAR-Data/","content":"\n# P1 - NuScenes Dataset Introduction\n\n## Why NuScenes?\n\nWith increasing inverstment in fields of autonomous driving, tech-companies are creating their own dataset for training their autonomous vehicles. Some of them, like Waymo Dataset, Lyft L5 Dataset and NuScenes Data are widely used in personal uncommercial reasearchs because of their open-source and mutimodal features.\n\nNuScenes Dataset as one of the earliest published dataset in this field contains various kinds of data collected by different sensors such as cameras, radar and LiDARs. It provides toolkit to help researchers easily and quickly get an overview and process the data with provided functions. And Lyft L5 Dataset also uses similiar toolkit.\n\nThus, let's start with processing NuScenes Dataset. Since we are focusing on deep learning with LiDAR data, which means we have to prepar the LiDAR data for our deep learning neural network, we'll first take an overview of the most important factors of dataset:\n\n+ Size\n+ Variety of Scenes\n+ Number and Quality of Annotated Objects\n\n|Dataset|Scenes|Size|PCs lidar|Ann.Frames|3D boxes|\n|----|----|----|----|----|----|\n|NuScenes|1k|5.5hr|400k|40k|1.4M|\n|Lyft L5|366|2.5hr|46k|46k|1.3M|\n|Waymo Open|1k|5.5hr|200k|200k|12M|\n\nAs the table shown above, NuScenes has a larger data size comparing to Lyft L5, and has an easy-to-use toolkit comparing to Waymo Dataset. For training a neural network, the more data we have, the better performance we could achieve. And with more different driving scenes and more different objects classes, the neural network will become robuster after training.\n\n## Toolkit(Devkit) Installation\n\nDevkit provides us various functions to extract the specific data format and data information from dataset. It also contains different matricies for evaluating NN(Neural Network)'s performance with respect to your goals such as prediction, detecton and so on.\n\n+ visit [official website](https://www.nuscenes.org/) and account registration\n+ Download Full dataset(v1.0): Trainval and Test Set\n+ Devkit Installation (for more information please visit their [github](https://github.com/nutonomy/nuscenes-devkit))\n\n> ```bash\n> # under Ubuntu or MacOS\n> # if you do not have python 3.6/3.7\n> # install python first\n> sudo apt install python-pip\n> sudo add-apt-repository ppa:deadsnakes/ppa\n> sudo apt-get update\n> sudo apt-get install python3.7\n> sudo apt-get install python3.7-dev\n> # then create virtual environment via conda or virtualenvwrapper\n> # if you do not have miniconda, google and install it\n> # after miniconda installed\n> conda create --name deep-learning-nuscenes python=3.7\n> conda activate deep-learning-nuscenes\n> # to deactivate env, use `source deactivate`\n> # then use\n> pip install nuscenes-devkit\n> ```\n\n+ Verify the installation and follow the tutorials [here](https://www.nuscenes.org/nuscenes?tutorial=nuscenes).\n\n## What could we do with Dev-Kit?\n\nWith devkit of nuscenes we could get an overview of the whole dataset and understand the data formats and structure inside the dataset. We would also use the functions, which devkit provides us, in future steps to build a dataset pre-processing pipeline.\n\nAs the introduction, let us try some in jupyter notebook to get better understand of the data formats of the dataset.\n\nTo receive an overview of the dataset:\n\n```python\nfrom nuscenes.nuscenes import NuScenes\n# if you download the mini-dataset\n# use version=\"v1.0-mini\"\n# dataroot should be the path\n# where you store your dataset\nnusc = NuScenes(version = \"v1.0-trainval\", dataroot=\"/home/ken/Data/Dataset/NuScenes\")\n\n# output:\n# ======\n# Loading NuScenes tables for version v1.0-trainval...\n# 23 category,\n# 8 attribute,\n# 4 visibility,\n# 64386 instance,\n# 12 sensor,\n# 10200 calibrated_sensor,\n# 2631083 ego_pose,\n# 68 log,\n# 850 scene,\n# 34149 sample,\n# 2631083 sample_data,\n# 1166187 sample_annotation,\n# 4 map,\n# Done loading in 37.807 seconds.\n# ======\n# Reverse indexing ...\n# Done reverse indexing in 6.4 seconds.\n# ======\n```\n\nAs above shown, we could see there are several data types in this dataset, e.g. `category`, `attribute`, `visibility`, `instance`, `sensor`, `calibrated_sensor`, `ego_pose`, `scene`, ... and so on. You can find the official definition [here](https://www.nuscenes.org/nuscenes#data-format).\n\nFor our focus, `scene`, `sample`, `sample_data`, `sample_annotation` will be important.\n\n1. scene\n\n   To get information of scenes:\n\n   ```python\n   nusc.list_scenes()\n   # output:\n   # scene-0161, Car overtaking, parking lot, peds, ped ... [18-05-21 15:07:23]   19s, boston-seaport, anns:1970\n   # scene-0162, Leaving parking lot, parked cars, hidde... [18-05-21 15:07:43]   19s, boston-seaport, anns:2230\n   # ...\n   ```\n\n   Watching these outputs, we could know the scenes contain different driving scenarios and each scene lasts about 20 seconds.\n\n   For a specific scene:\n\n   ```python\n   my_scene = nusc.scene[0]\n   my_scene\n   # output:\n   # {'token': '73030fb67d3c46cfb5e590168088ae39',\n   # 'log_token': '6b6513e6c8384cec88775cae30b78c0e',\n   # 'nbr_samples': 40,\n   # 'first_sample_token': 'e93e98b63d3b40209056d129dc53ceee',\n   # 'last_sample_token': '40e413c922184255a94f08d3c10037e0',\n   # 'name': 'scene-0001',\n   # 'description': 'Construction, maneuver between several trucks'}\n   \n   # explanation:\n   # 'token' -- Code for searching and calling this scene\n   # 'log_token' -- Code for searching and calling this scene's log\n   # 'nbr_samples' -- number of samples in this scene\n   # 'first_sample_token/last_sample_token' -- Code for searching and calling this scene's first/last frame\n   # 'name', 'decription' -- other information\n   ```\n\n2. sample\n\n   A sample means a frame. In this dataset, the data is collected every 0.1 second(10 Hz), and it is annotated every half a second(2 Hz), which means every 5 frames, we would have 1 annotated sample. In annotated sample, the existed objects will be annotated.\n\n   Let us take a sample as example:\n\n   ```python\n   first_sample_token = my_scene['first_sample_token']\n   my_sample = nusc.get('sample', first_sample_token)\n   my_sample\n\n   # output:\n   # {'token': 'e93e98b63d3b40209056d129dc53ceee',\n   # 'timestamp': 1531883530449377,\n   # 'prev': '',\n   # 'next': '14d5adfe50bb4445bc3aa5fe607691a8',\n   # 'scene_token': '73030fb67d3c46cfb5e590168088ae39',\n   # 'data': {'RADAR_FRONT': 'bddd80ae33ec4e32b27fdb3c1160a30e',\n   # 'RADAR_FRONT_LEFT': '1a08aec0958e42ebb37d26612a2cfc57',\n   # 'RADAR_FRONT_RIGHT': '282fa8d7a3f34b68b56fb1e22e697668',\n   # 'RADAR_BACK_LEFT': '05fc4678025246f3adf8e9b8a0a0b13b',\n   # 'RADAR_BACK_RIGHT': '31b8099fb1c44c6381c3c71b335750bb',\n   # 'LIDAR_TOP': '3388933b59444c5db71fade0bbfef470',\n   # 'CAM_FRONT': '020d7b4f858147558106c504f7f31bef',\n   # 'CAM_FRONT_RIGHT': '16d39ff22a8545b0a4ee3236a0fe1c20',\n   # 'CAM_BACK_RIGHT': 'ec7096278e484c9ebe6894a2ad5682e9',\n   # 'CAM_BACK': 'aab35aeccbda42de82b2ff5c278a0d48',\n   # 'CAM_BACK_LEFT': '86e6806d626b4711a6d0f5015b090116',\n   # 'CAM_FRONT_LEFT': '24332e9c554a406f880430f17771b608'},\n   # 'anns': ['173a50411564442ab195e132472fde71',\n   # '5123ed5e450948ac8dc381772f2ae29a',\n   # 'acce0b7220754600b700257a1de1573d',\n   # '8d7cb5e96cae48c39ef4f9f75182013a',\n   # 'f64bfd3d4ddf46d7a366624605cb7e91',\n   # 'f9dba7f32ed34ee8adc92096af767868',\n   # '086e3f37a44e459987cde7a3ca273b5b',\n   # '3964235c58a745df8589b6a626c29985',\n   # '31a96b9503204a8688da75abcd4b56b2',\n   # 'b0284e14d17a444a8d0071bd1f03a0a2']}\n\n   # explanation:\n   # 'token' -- Code for searching and calling this sample\n   # 'timestamp' -- the timestamp of this frame\n   # 'prev'/'next' -- the frame of previous/next frame(before/after 0.1 sec).\n   # `scene` -- the token of scene, which this sample belongs to.\n   # `data` -- the token of data, which different sensors collect at this time frame, we could see different radar, lidar and cameras\n   # `anns` -- the token of annotated objects in this frame\n   ```\n\n3. sample_data\n\n   sample data means the data collected by a specific sensor in a frame. In part of sample, we could know the dataset contains data collected by RADAR, LiDAR, and Cameras. Let us take look how lidar data looks like:\n\n   ```python\n   sensor = 'LIDAR_TOP'\n   lidar_data = nusc.get('sample_data', my_sample['data'][sensor])\n   lidar_data\n\n   # output:\n   # {'token': '3388933b59444c5db71fade0bbfef470',\n   # 'sample_token': 'e93e98b63d3b40209056d129dc53ceee',\n   # 'ego_pose_token': '3388933b59444c5db71fade0bbfef470',\n   # 'calibrated_sensor_token': '7a0cd258d096410eb68251b4b87febf5',\n   # 'timestamp': 1531883530449377,\n   # 'fileformat': 'pcd',\n   # 'is_key_frame': True,\n   # 'height': 0,\n   # 'width': 0,\n   # 'filename': 'samples/LIDAR_TOP/n015-2018-07-18-11-07-57+0800__LIDAR_TOP__1531883530449377.pcd.bin',\n   # 'prev': '',\n   # 'next': 'bc2cd87d110747cd9849e2b8578b7877',\n   # 'sensor_modality': 'lidar',\n   # 'channel': 'LIDAR_TOP'}\n\n   # explanation:\n   # a set of 'token' -- code for corresponding object\n   # 'timestamp' -- the timestamp of collection\n   # 'fileformat' -- here the lidar data is in point cloud format\n   # 'is_key_frame' -- keyframe means the frame is annotated\n   # 'heigt'/'width' -- 3D lidar data does not have these attributes\n   # 'filename' -- path of the stored data\n   # 'prev'/'next' -- token of data in previous/next frame\n\n   ```\n\n   For visualization of the lidar data:\n\n   ```python\n   nusc.render_sample_data(lidar_data['token'])\n   ```\n\n   ![](https://raw.githubusercontent.com/adskenlin/adskenlin.github.io/master/2021/08/16/Deep-Learning-on-NuScenes-LiDAR-Data/lidar_vis.png)\n\n4. sample_annotation\n\n   sample_annotation refers to the anntated information of an object in a sample.\n\n   ```python\n   my_annotation_token = my_sample['anns'][5]\n   my_annotation_metadata =  nusc.get('sample_annotation', my_annotation_token)\n   my_annotation_metadata\n\n   # output:\n   # {'token': 'f9dba7f32ed34ee8adc92096af767868',\n   # 'sample_token': 'e93e98b63d3b40209056d129dc53ceee',\n   # 'instance_token': '076e76a589dd4f40adce27b3f3377f58',\n   # 'visibility_token': '4',\n   # 'attribute_tokens': ['cb5118da1ab342aa947717dc53544259'],\n   # 'translation': [1009.009, 598.528, 0.664],\n   # 'size': [1.871, 4.478, 1.456],\n   # 'rotation': [0.8844059033120215, 0.0, 0.0, 0.46671854279302766],\n   # 'prev': '',\n   # 'next': '21ece7170dfa431bb504e15f68fc40ce',\n   # 'num_lidar_pts': 151,\n   # 'num_radar_pts': 3,\n   # 'category_name': 'vehicle.car'}\n\n   # explanation:\n   # a set of token -- code for corresponding object\n   # 'translation' -- the global coordinate system of center of this object\n   # 'size' -- the size(width, length, height) of this object\n   # 'rotation' -- the orientation of the object shown in quaternion\n   # 'num_lidar_pts' -- number of lidar points on this object\n   # 'category_name' -- object class name \n   ```\n\n   For visulization of this object:\n\n   ```python\n   nusc.render_annotation(my_annotation_token)\n   ```\n\n   ![](https://raw.githubusercontent.com/adskenlin/adskenlin.github.io/master/2021/08/16/Deep-Learning-on-NuScenes-LiDAR-Data/anns_vis.png)\n\n## Data Format and Usage\n\nAs above shown, we've introduced different data formats in NuScenes dataset. The data formats and their corresponding functions could help us build data processing pipeline in future steps.\n\nBefore we build the data pre-processing pipeline, we need to think about: What kind of LiDAR data do we need for deep learning? An example of senor data processing from SOTA methods like Center-Net, MultiXNet shows the pipeline as follows: \n\n![](https://raw.githubusercontent.com/adskenlin/adskenlin.github.io/master/2021/08/16/Deep-Learning-on-NuScenes-LiDAR-Data/lidardata_pipeline.PNG)\n\nWe'll discuss data pre-processing methods and options in next part.\n\n## Reference\n\n[1] [nuScenes: A multimodal dataset for autonomous driving](https://arxiv.org/pdf/1903.11027.pdf)\n\n[2] [Center-based 3D Object Detection and Tracking](https://arxiv.org/pdf/2006.11275.pdf)\n","tags":["Deep Learning","NuScenes","LiDAR"]},{"title":"Solution to Problems You May Meet With Parallel System Ubuntu","url":"/2021/08/15/Solution-to-Problems-You-May-Meet-With-Parallel-System-Linux/","content":"This article collects the problems I met while I was installing Ubuntu and using Ubuntu at the beginning.\n## Normal Steps to Install Linux(Ubuntu)\n+ Download Ubuntu from [official website](https://ubuntu.com/download/desktop).\n+ Burn the .msi to your U-Disk with [Rufus](http://rufus.ie/en/).\n+ Prapare the Disk partition for Ubuntu System:\nWindows' Disk Management -> Right Click on your available Disk -> Shrink Volume... -> Set the space -> Shrink\n+ Turn off Win10 Quick Start\n+ Restart the PC, stick your U-Disk, go to BIOS Setting\n+ Choose \"UEFI: USB Flash Disk\" in start menu\n+ Choose \"Install Ubuntu\"\n+ Finish Installation and restart\n+ If your PC automatically use Windows, go to BIOS Setting again and choose ubuntu to start\n\n## Problems\n+ When you switch your system, the system time will be incorrect:\n\n    ```bash\n    // Solution: Change UTC=yes to UTC=no in Linux \n    // type in cmd:\n    timedatectl set-local-rtc 1 --adjust-system-clock\n    ```\n\n+ Installation freezes at the start and show checking nvidia driver:\n\n    ```txt\n    Solution: \n    1. when you see \"install Ubuntu\" etc.. (GRUB) press \"e\" quickly! \n    2. Edit: Replace \"quiet splash\" to \"nomodeset\" and press F10 to boot.\n    3. After installation done, and reboot.\n    4. At the same place(GRUB), press \"e\" and in the line that starts with \"linux\", add \"nouveau.modeset=0\" at the end of that line. \n    5. When you successfully enter Ubuntu, use Software and Update to install Nvidia Driver.\n    ```\n\n+ Connect Window's Disk in Ubuntu:\n    ```txt\n    Solution:\n    1. with `sudo fdisk -l` you could get an overview of all disk partitions.\n    2. mark the name of device used by your windows disk, e.g. /dev/sda2.\n    3. `sudo gedit /etc/fstab`\n    4. add a line at the end with e.g.\n    `/dev/sda2  /media/Data`\n    5. the device would be able to visited through `/media/Data` in your ubuntu system\n    ```\n","tags":["Ubuntu"]},{"title":"CMake VS Code Setup on Windows","url":"/2021/08/13/CMake-VS-Code-Setup-on-Windows/","content":"# CMake VS Code Setup on Windows\nIn this article, I would like to introduce how to use CMake with VS Code on Windows.\n## What's CMake\n----\nCMake(Cross Platform Make) is free and open-source software for build automation, testing, packaging and installation of software by using a compiler-independent method. It's widely used in software projects. CMake offers higher-level mechanisms like external library detection and configuration (i.e. automatically setting up the defines, include directories and link files for working with a 3rd party library), support for generating installation (and packaging), integration with the CTest test utility and so on.\n\n\n## Installation\n----\nOn Windows, CMake could be directly downloaded und installed with their [official website](https://cmake.org/download/).\nAfter Installation, in order to use `cmake` in cmd, you have to add the path to your environments' variables following:\n\n\n    1. Click Start, type Accounts in the Start search box, and then click User Accounts under Programs.\n\n    2. If you are prompted for an administrator password or for a confirmation, type the password, or click Allow.\n\n    3. In the User Accounts dialog box, click Change my environment variables under Tasks.\n\n    4. Make the changes that you want to the user environment variables for your user account, and then click OK.\nTo verify installation, you could type `cmake --version` in your cmd/powershell.\n\nIn VS Code, make sure the `Extensions` named `CMake` and `CMake Tools` successfully installed.\n\n## Setup\n----\nAt the beginning, create a folder and use VS Code to open it. We'll use this folder to test the functions of CMake.  \n\nAdditionally, make sure you've correct configuration for compiler path in `C/C++: Edit Configurations(UI)`(use ctrl+shift+P and type in). In my case, the path is `C:/msys64/mingw64/bin/gcc.exe`.\n\nIt's highly recommanded that a C++ project with CMake use the folder architecture as follows:\n```bash\n|- test_folder\n    |--.vscode\n    |    |-- c_cpp_properties.json\n    |    |-- tasks.json\n    |    â””-- launch.json\n    |-- build\n    |-- include\n    |    â””-- your-head-files\n    |-- src\n    |    â””-- source_code.cpp\n    â””-- CMakeLists.txt\n```\nif you don't know how to get `c_cpp_properties.json`, `tasks.json`, `launch.json`, please refer to [here](https://code.visualstudio.com/docs/cpp/config-msvc) and [Debug using launch.json](https://code.visualstudio.com/docs/cpp/launch-json-reference) for help.\n\nThe core part is how to write the `CMakeLists.txt`:\nActually `cmake` is using `CMakeLists.txt` to execute build. A simple example:\n```cmake\ncmake_minimum_required (VERSION 2.8...3.21.1)\n\nproject(project_name)\n\ninclude_directories(${CMAKE_SOURCE_DIR}/include)\n\naux_source_directory(${CMAKE_SOURCE_DIR}/src DIR_SRC)\n\nadd_executable (project_name ${DIR_SRC})\n```\nThe variables and commands for `CMakeLists.txt` will be introduced in next part.\n\nNow we have all requirements for cmake, let's try:\n```bash\ncd build\ncmake ..\nmake\n```\nIf you don't have `make` installed, here is a short cut to make it work:\n1. Go to the folder where you install you compiler MinGW, in my case it's under `C:\\msys64\\mingw64\\bin`.\n2. Find a file named `mingw32-make.exe` and make a copy in the same foler.\n3. Rename the copy to `make.exe`.\n4. Check with `make --version` in cmd or powershell.\n\nIf the codes above work, you'll see *.exe created under `/build`, cmake works successfully.\n\n## Useful Variables and Commands for CMakeLists.txt\n----\nThe meaning of following words are able to be found under this [documentaion](https://cmake.org/cmake/help/latest/index.html)\n+ Variables: \n    ```cmake\n    CMAKE_SOURCE_DIR\n\n    PROJECT_SOURCE_DIR\n\n    CMAKE_CURRENT_SOURCE_DIR\n\n    CMAKE_MODULE_PATH\n\n    EXECUTABLE_OUTPUT_PATH\n\n    LIBRARY_OUTPUT_PATH\n\n    PROJECT_NAME\n    ```\n+ Commands:\n    ```cmake\n    // setup project name and supported progamming language\n    project(projectname [CXX] [C] [Java])\n\n    // setup the supported cmake version \n    CMAKE_MINIMUM_REQUIRED(VERSION versionNumber [FATAL_ERROR])\n\n    // setup variables\n    SET(VAR [VALUE] [CACHE TYPE DOCSTRING [FORCE]])\n    set(EXECUTABLE_OUTPUT_PATH ${PROJECT_BINARY_DIR}/bin)\n\n    // find all source code files and make them into a list\n    aux_source_directory(dir VARIABLE)\n\n    // add subdirectory to the build\n    add_subdirectory(source_dir [binary_dir][EXCLUDE_FROM_ALL])\n\n    // find head files\n    include_directories([AFTER|BEFORE] [SYSTEM] dir1 dir2 ...)\n\n    // Adds an executable target called <name> to be built from the source files listed\n    add_executable(<name> [source1] [source2 ...])\n\n    // ...\n    ```\n## Reference:\n[1] [CMake Wiki](https://en.wikipedia.org/wiki/CMake) \\\n[2] [What are the benefits/purposes of cmake?](https://stackoverflow.com/questions/19686223/what-are-the-benefits-purposes-of-cmake) \\\n[3] [CMake Documentation](https://cmake.org/cmake/help/latest/index.html#)\n\n\n\n","tags":["CMake","VS Code"]},{"title":"How to Create Personal Blog on github Using Hexo","url":"/2021/08/11/personal_blogs/","content":"\n## Dependencies\n+ Node.js\n+ npm\n+ Git\n\n\n## Install Hexo\nif you don't have `node.js` and npm `npm` on your workstation, please visit [official site](https://nodejs.org/en/) to install the node.js. (npm would be automatically installed if node.js installation is done)\nTo check whether node.js and npm are successfully installed:\n```bash\n# in cmd/terminal\nnode -v # v14.17.3 or others\nnpm -v # 6.14.13 or others\n``` \nIf you are using win10 as your operate system and get errors with these codes, you should re-open the powershell/cmd as administor after the installation then try the codes above.\n\nThen:\n```bash\nnpm install -g hexo-cli\n```\nTo check whether hexo successfully installed:\n```bash\nhexo -v\n```\nYou'll see the version information e.g.:\n```bash\n hexo-cli:4.3.0, \n node:14.17.3, \n ... \n```\n\n## Create Blogs\nFirst of all, you have to navigate to the the folder, where you plan to create your blogs' folder.\n```bash\ncd E:\\\nmkdir my_blogs\ncd my_blogs\n```\nThen run:\n```bash\n# if you are using powershell as administor on windows,\nhexo init\n# if you are on mac/linux, \nsudo hexo init\n```\nCreate your first post:\n```bash\nhexo new \"My First Post\"\n```\nA new file with .md format will be created in `source/_posts/`. You could directly edit it in file. Also, you could preview your blog on your local machine with:\n```bash\n#establish a localhost for blog\nhexo server\n\n# if you have some updates to your blogs, use\nhexo clean\nhexo generate\nhexo server\n```\nThe response will show you a addresse like `http://localhost:4000`, you could visit it on local.\n\n\n## Deploy Blogs\nAfter you know how to write and make changes to your blogs on local, we'll introduce how to create a github page and deploy your blog to it. Install Deloyer with:\n```bash\nnpm install -save hexo-deployer-git\n``` \nGo to your github and create a new repositery called `username.github.io`. `username` should be your github username. This will be used as the address of github page later.\n\nOn your local, you need to edit the `_config.yml`. You could find this file in the blog folder. `_config.yml` is the key to set up your blogs, such as set up the name, discription, theme and so on. \n```bash\n# in _config.yml, find the part #Deployment change the info like below\ndeploy:\n  type: 'git'\n  repo: your-repo-adress\n  branch: master\n```\nAfter this, in your command line type:\n```bash\nhexo deploy\n```\nAfter a while, you would be able to visit `username.github.io` to see your own blog.\n\n## Change the Theme of Your Blog\nThere are varous free themes on [Hexo's official site](https://hexo.io/themes/). You could pick one to visit its github repo and follow the instruction to install it.\n\nE.g. I'd like to change the theme of my blog, firstly, \n```bash\n# in the path of your blog folder, clone the theme to local\ngit clone `url_of_the_theme_repo` themes/my_theme\n```\nThen modify the `_config.yml`. (most theme repos have demo, you could directly compare the `_config.yml` to your own and make some changes)\n\nAfter this:\n```bash\nhexo clean\nhexo generate\n# take a preview on local, \nhexo server\n# if everything is fine,\nhexo deploy\n```\n\n\n\nyou've successfully created your own blog! Congrats!","tags":["Hexo","Blog"]}]