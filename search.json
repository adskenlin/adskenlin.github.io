[{"title":"Deep Learning on LiDAR Data P1-Dataset","url":"/2021/08/16/Deep-Learning-on-NuScenes-LiDAR-Data/","content":"\n# P1 - NuScenes Dataset Introduction\n\n## Why NuScenes?\n\nWith increasing inverstment in fields of autonomous driving, tech-companies are creating their own dataset for training their autonomous vehicles. Some of them, like Waymo Dataset, Lyft L5 Dataset and NuScenes Data are widely used in personal uncommercial reasearchs because of their open-source and mutimodal features.\n\nNuScenes Dataset as one of the earliest published dataset in this field contains various kinds of data collected by different sensors such as cameras, radar and LiDARs. It provides toolkit to help researchers easily and quickly get an overview and process the data with provided functions. And Lyft L5 Dataset also uses similiar toolkit.\n\nThus, let's start with processing NuScenes Dataset. Since we are focusing on deep learning with LiDAR data, which means we have to prepar the LiDAR data for our deep learning neural network, we'll first take an overview of the most important factors of dataset:\n\n+ Size\n+ Variety of Scenes\n+ Number and Quality of Annotated Objects\n\n|Dataset|Scenes|Size|PCs lidar|Ann.Frames|3D boxes|\n|----|----|----|----|----|----|\n|NuScenes|1k|5.5hr|400k|40k|1.4M|\n|Lyft L5|366|2.5hr|46k|46k|1.3M|\n|Waymo Open|1k|5.5hr|200k|200k|12M|\n\nAs the table shown above, NuScenes has a larger data size comparing to Lyft L5, and has an easy-to-use toolkit comparing to Waymo Dataset. For training a neural network, the more data we have, the better performance we could achieve. And with more different driving scenes and more different objects classes, the neural network will become robuster after training.\n\n## Toolkit(Devkit) Installation\n\nDevkit provides us various functions to extract the specific data format and data information from dataset. It also contains different matricies for evaluating NN(Neural Network)'s performance with respect to your goals such as prediction, detecton and so on.\n\n+ visit [official website](https://www.nuscenes.org/) and account registration\n+ Download Full dataset(v1.0): Trainval and Test Set\n+ Devkit Installation (for more information please visit their [github](https://github.com/nutonomy/nuscenes-devkit))\n\n> ```bash\n> # under Ubuntu or MacOS\n> # if you do not have python 3.6/3.7\n> # install python first\n> sudo apt install python-pip\n> sudo add-apt-repository ppa:deadsnakes/ppa\n> sudo apt-get update\n> sudo apt-get install python3.7\n> sudo apt-get install python3.7-dev\n> # then create virtual environment via conda or virtualenvwrapper\n> # if you do not have miniconda, google and install it\n> # after miniconda installed\n> conda create --name deep-learning-nuscenes python=3.7\n> conda activate deep-learning-nuscenes\n> # to deactivate env, use `source deactivate`\n> # then use\n> pip install nuscenes-devkit\n> ```\n\n+ Verify the installation and follow the tutorials [here](https://www.nuscenes.org/nuscenes?tutorial=nuscenes).\n\n## What could we do with Dev-Kit?\n\nWith devkit of nuscenes we could get an overview of the whole dataset and understand the data formats and structure inside the dataset. We would also use the functions, which devkit provides us, in future steps to build a dataset pre-processing pipeline.\n\nAs the introduction, let us try some in jupyter notebook to get better understand of the data formats of the dataset.\n\nTo receive an overview of the dataset:\n\n```python\nfrom nuscenes.nuscenes import NuScenes\n# if you download the mini-dataset\n# use version=\"v1.0-mini\"\n# dataroot should be the path\n# where you store your dataset\nnusc = NuScenes(version = \"v1.0-trainval\", dataroot=\"/home/ken/Data/Dataset/NuScenes\")\n\n# output:\n# ======\n# Loading NuScenes tables for version v1.0-trainval...\n# 23 category,\n# 8 attribute,\n# 4 visibility,\n# 64386 instance,\n# 12 sensor,\n# 10200 calibrated_sensor,\n# 2631083 ego_pose,\n# 68 log,\n# 850 scene,\n# 34149 sample,\n# 2631083 sample_data,\n# 1166187 sample_annotation,\n# 4 map,\n# Done loading in 37.807 seconds.\n# ======\n# Reverse indexing ...\n# Done reverse indexing in 6.4 seconds.\n# ======\n```\n\nAs above shown, we could see there are several data types in this dataset, e.g. `category`, `attribute`, `visibility`, `instance`, `sensor`, `calibrated_sensor`, `ego_pose`, `scene`, ... and so on. You can find the official definition [here](https://www.nuscenes.org/nuscenes#data-format).\n\nFor our focus, `scene`, `sample`, `sample_data`, `sample_annotation` will be important.\n\n1. scene\n\n   To get information of scenes:\n\n   ```python\n   nusc.list_scenes()\n   # output:\n   # scene-0161, Car overtaking, parking lot, peds, ped ... [18-05-21 15:07:23]   19s, boston-seaport, anns:1970\n   # scene-0162, Leaving parking lot, parked cars, hidde... [18-05-21 15:07:43]   19s, boston-seaport, anns:2230\n   # ...\n   ```\n\n   Watching these outputs, we could know the scenes contain different driving scenarios and each scene lasts about 20 seconds.\n\n   For a specific scene:\n\n   ```python\n   my_scene = nusc.scene[0]\n   my_scene\n   # output:\n   # {'token': '73030fb67d3c46cfb5e590168088ae39',\n   # 'log_token': '6b6513e6c8384cec88775cae30b78c0e',\n   # 'nbr_samples': 40,\n   # 'first_sample_token': 'e93e98b63d3b40209056d129dc53ceee',\n   # 'last_sample_token': '40e413c922184255a94f08d3c10037e0',\n   # 'name': 'scene-0001',\n   # 'description': 'Construction, maneuver between several trucks'}\n   \n   # explanation:\n   # 'token' -- Code for searching and calling this scene\n   # 'log_token' -- Code for searching and calling this scene's log\n   # 'nbr_samples' -- number of samples in this scene\n   # 'first_sample_token/last_sample_token' -- Code for searching and calling this scene's first/last frame\n   # 'name', 'decription' -- other information\n   ```\n\n2. sample\n\n   A sample means a frame. In this dataset, the data is collected every 0.1 second(10 Hz), and it is annotated every half a second(2 Hz), which means every 5 frames, we would have 1 annotated sample. In annotated sample, the existed objects will be annotated.\n\n   Let us take a sample as example:\n\n   ```python\n   first_sample_token = my_scene['first_sample_token']\n   my_sample = nusc.get('sample', first_sample_token)\n   my_sample\n\n   # output:\n   # {'token': 'e93e98b63d3b40209056d129dc53ceee',\n   # 'timestamp': 1531883530449377,\n   # 'prev': '',\n   # 'next': '14d5adfe50bb4445bc3aa5fe607691a8',\n   # 'scene_token': '73030fb67d3c46cfb5e590168088ae39',\n   # 'data': {'RADAR_FRONT': 'bddd80ae33ec4e32b27fdb3c1160a30e',\n   # 'RADAR_FRONT_LEFT': '1a08aec0958e42ebb37d26612a2cfc57',\n   # 'RADAR_FRONT_RIGHT': '282fa8d7a3f34b68b56fb1e22e697668',\n   # 'RADAR_BACK_LEFT': '05fc4678025246f3adf8e9b8a0a0b13b',\n   # 'RADAR_BACK_RIGHT': '31b8099fb1c44c6381c3c71b335750bb',\n   # 'LIDAR_TOP': '3388933b59444c5db71fade0bbfef470',\n   # 'CAM_FRONT': '020d7b4f858147558106c504f7f31bef',\n   # 'CAM_FRONT_RIGHT': '16d39ff22a8545b0a4ee3236a0fe1c20',\n   # 'CAM_BACK_RIGHT': 'ec7096278e484c9ebe6894a2ad5682e9',\n   # 'CAM_BACK': 'aab35aeccbda42de82b2ff5c278a0d48',\n   # 'CAM_BACK_LEFT': '86e6806d626b4711a6d0f5015b090116',\n   # 'CAM_FRONT_LEFT': '24332e9c554a406f880430f17771b608'},\n   # 'anns': ['173a50411564442ab195e132472fde71',\n   # '5123ed5e450948ac8dc381772f2ae29a',\n   # 'acce0b7220754600b700257a1de1573d',\n   # '8d7cb5e96cae48c39ef4f9f75182013a',\n   # 'f64bfd3d4ddf46d7a366624605cb7e91',\n   # 'f9dba7f32ed34ee8adc92096af767868',\n   # '086e3f37a44e459987cde7a3ca273b5b',\n   # '3964235c58a745df8589b6a626c29985',\n   # '31a96b9503204a8688da75abcd4b56b2',\n   # 'b0284e14d17a444a8d0071bd1f03a0a2']}\n\n   # explanation:\n   # 'token' -- Code for searching and calling this sample\n   # 'timestamp' -- the timestamp of this frame\n   # 'prev'/'next' -- the frame of previous/next frame(before/after 0.1 sec).\n   # `scene` -- the token of scene, which this sample belongs to.\n   # `data` -- the token of data, which different sensors collect at this time frame, we could see different radar, lidar and cameras\n   # `anns` -- the token of annotated objects in this frame\n   ```\n\n3. sample_data\n\n   sample data means the data collected by a specific sensor in a frame. In part of sample, we could know the dataset contains data collected by RADAR, LiDAR, and Cameras. Let us take look how lidar data looks like:\n\n   ```python\n   sensor = 'LIDAR_TOP'\n   lidar_data = nusc.get('sample_data', my_sample['data'][sensor])\n   lidar_data\n\n   # output:\n   # {'token': '3388933b59444c5db71fade0bbfef470',\n   # 'sample_token': 'e93e98b63d3b40209056d129dc53ceee',\n   # 'ego_pose_token': '3388933b59444c5db71fade0bbfef470',\n   # 'calibrated_sensor_token': '7a0cd258d096410eb68251b4b87febf5',\n   # 'timestamp': 1531883530449377,\n   # 'fileformat': 'pcd',\n   # 'is_key_frame': True,\n   # 'height': 0,\n   # 'width': 0,\n   # 'filename': 'samples/LIDAR_TOP/n015-2018-07-18-11-07-57+0800__LIDAR_TOP__1531883530449377.pcd.bin',\n   # 'prev': '',\n   # 'next': 'bc2cd87d110747cd9849e2b8578b7877',\n   # 'sensor_modality': 'lidar',\n   # 'channel': 'LIDAR_TOP'}\n\n   # explanation:\n   # a set of 'token' -- code for corresponding object\n   # 'timestamp' -- the timestamp of collection\n   # 'fileformat' -- here the lidar data is in point cloud format\n   # 'is_key_frame' -- keyframe means the frame is annotated\n   # 'heigt'/'width' -- 3D lidar data does not have these attributes\n   # 'filename' -- path of the stored data\n   # 'prev'/'next' -- token of data in previous/next frame\n\n   ```\n\n   For visualization of the lidar data:\n\n   ```python\n   nusc.render_sample_data(lidar_data['token'])\n   ```\n\n   ![](https://raw.githubusercontent.com/adskenlin/adskenlin.github.io/master/2021/08/16/Deep-Learning-on-NuScenes-LiDAR-Data/lidar_vis.png)\n\n4. sample_annotation\n\n   sample_annotation refers to the anntated information of an object in a sample.\n\n   ```python\n   my_annotation_token = my_sample['anns'][5]\n   my_annotation_metadata =  nusc.get('sample_annotation', my_annotation_token)\n   my_annotation_metadata\n\n   # output:\n   # {'token': 'f9dba7f32ed34ee8adc92096af767868',\n   # 'sample_token': 'e93e98b63d3b40209056d129dc53ceee',\n   # 'instance_token': '076e76a589dd4f40adce27b3f3377f58',\n   # 'visibility_token': '4',\n   # 'attribute_tokens': ['cb5118da1ab342aa947717dc53544259'],\n   # 'translation': [1009.009, 598.528, 0.664],\n   # 'size': [1.871, 4.478, 1.456],\n   # 'rotation': [0.8844059033120215, 0.0, 0.0, 0.46671854279302766],\n   # 'prev': '',\n   # 'next': '21ece7170dfa431bb504e15f68fc40ce',\n   # 'num_lidar_pts': 151,\n   # 'num_radar_pts': 3,\n   # 'category_name': 'vehicle.car'}\n\n   # explanation:\n   # a set of token -- code for corresponding object\n   # 'translation' -- the global coordinate system of center of this object\n   # 'size' -- the size(width, length, height) of this object\n   # 'rotation' -- the orientation of the object shown in quaternion\n   # 'num_lidar_pts' -- number of lidar points on this object\n   # 'category_name' -- object class name \n   ```\n\n   For visulization of this object:\n\n   ```python\n   nusc.render_annotation(my_annotation_token)\n   ```\n\n   ![](https://raw.githubusercontent.com/adskenlin/adskenlin.github.io/master/2021/08/16/Deep-Learning-on-NuScenes-LiDAR-Data/lidar_vis.png)\n\n## Data Format and Usage\n\nAs above shown, we've introduced different data formats in NuScenes dataset. The data formats and their corresponding functions could help us build data processing pipeline in future steps.\n\nBefore we build the data pre-processing pipeline, we need to think about: What kind of LiDAR data do we need for deep learning? An example of senor data processing from SOTA methods like Center-Net, MultiXNet shows the pipeline as follows: \n\n![](https://raw.githubusercontent.com/adskenlin/adskenlin.github.io/master/2021/08/16/Deep-Learning-on-NuScenes-LiDAR-Data/lidardata_pipeline.PNG)\n\nWe'll discuss data pre-processing methods and options in next part.\n\n## Reference\n\n[1] [nuScenes: A multimodal dataset for autonomous driving](https://arxiv.org/pdf/1903.11027.pdf)\n\n[2] [Center-based 3D Object Detection and Tracking](https://arxiv.org/pdf/2006.11275.pdf)\n","tags":["NuScenes","Deep Learning","LiDAR"]},{"title":"Solution to Problems You May Meet With Parallel System Ubuntu","url":"/2021/08/15/Solution-to-Problems-You-May-Meet-With-Parallel-System-Linux/","content":"This article collects the problems I met while I was installing Ubuntu and using Ubuntu at the beginning.\n## Normal Steps to Install Linux(Ubuntu)\n+ Download Ubuntu from [official website](https://ubuntu.com/download/desktop).\n+ Burn the .msi to your U-Disk with [Rufus](http://rufus.ie/en/).\n+ Prapare the Disk partition for Ubuntu System:\nWindows' Disk Management -> Right Click on your available Disk -> Shrink Volume... -> Set the space -> Shrink\n+ Turn off Win10 Quick Start\n+ Restart the PC, stick your U-Disk, go to BIOS Setting\n+ Choose \"UEFI: USB Flash Disk\" in start menu\n+ Choose \"Install Ubuntu\"\n+ Finish Installation and restart\n+ If your PC automatically use Windows, go to BIOS Setting again and choose ubuntu to start\n\n## Problems\n+ When you switch your system, the system time will be incorrect:\n\n```bash\n// Solution: Change UTC=yes to UTC=no in Linux \n// type in cmd:\ntimedatectl set-local-rtc 1 --adjust-system-clock\n```\n\n+ Installation freezes at the start and show checking nvidia driver:\n\n```txt\nSolution: \n1. when you see \"install Ubuntu\" etc.. (GRUB) press \"e\" quickly! \n2. Edit: Replace \"quiet splash\" to \"nomodeset\" and press F10 to boot.\n3. After installation done, and reboot.\n4. At the same place(GRUB), press \"e\" and in the line that starts with \"linux\", add \"nouveau.modeset=0\" at the end of that line. \n5. When you successfully enter Ubuntu, use Software and Update to install Nvidia Driver.\n```\n\n+ Connect Window's Disk in Ubuntu:\n```txt\nSolution:\n1. with `sudo fdisk -l` you could get an overview of all disk partitions.\n2. mark the name of device used by your windows disk, e.g. /dev/sda2.\n3. `sudo gedit /etc/fstab`\n4. add a line at the end with e.g.\n`/dev/sda2  /media/Data`\n5. the device would be able to visited through `/media/Data` in your ubuntu system\n```\n","tags":["Ubuntu"]},{"title":"CMake VS Code Setup on Windows","url":"/2021/08/13/CMake-VS-Code-Setup-on-Windows/","content":"# CMake VS Code Setup on Windows\nIn this article, I would like to introduce how to use CMake with VS Code on Windows.\n## What's CMake\n----\nCMake(Cross Platform Make) is free and open-source software for build automation, testing, packaging and installation of software by using a compiler-independent method. It's widely used in software projects. CMake offers higher-level mechanisms like external library detection and configuration (i.e. automatically setting up the defines, include directories and link files for working with a 3rd party library), support for generating installation (and packaging), integration with the CTest test utility and so on.\n\n\n## Installation\n----\nOn Windows, CMake could be directly downloaded und installed with their [official website](https://cmake.org/download/).\nAfter Installation, in order to use `cmake` in cmd, you have to add the path to your environments' variables following:\n\n\n    1. Click Start, type Accounts in the Start search box, and then click User Accounts under Programs.\n\n    2. If you are prompted for an administrator password or for a confirmation, type the password, or click Allow.\n\n    3. In the User Accounts dialog box, click Change my environment variables under Tasks.\n\n    4. Make the changes that you want to the user environment variables for your user account, and then click OK.\nTo verify installation, you could type `cmake --version` in your cmd/powershell.\n\nIn VS Code, make sure the `Extensions` named `CMake` and `CMake Tools` successfully installed.\n\n## Setup\n----\nAt the beginning, create a folder and use VS Code to open it. We'll use this folder to test the functions of CMake.  \n\nAdditionally, make sure you've correct configuration for compiler path in `C/C++: Edit Configurations(UI)`(use ctrl+shift+P and type in). In my case, the path is `C:/msys64/mingw64/bin/gcc.exe`.\n\nIt's highly recommanded that a C++ project with CMake use the folder architecture as follows:\n```bash\n|- test_folder\n    |--.vscode\n    |    |-- c_cpp_properties.json\n    |    |-- tasks.json\n    |    └-- launch.json\n    |-- build\n    |-- include\n    |    └-- your-head-files\n    |-- src\n    |    └-- source_code.cpp\n    └-- CMakeLists.txt\n```\nif you don't know how to get `c_cpp_properties.json`, `tasks.json`, `launch.json`, please refer to [here](https://code.visualstudio.com/docs/cpp/config-msvc) and [Debug using launch.json](https://code.visualstudio.com/docs/cpp/launch-json-reference) for help.\n\nThe core part is how to write the `CMakeLists.txt`:\nActually `cmake` is using `CMakeLists.txt` to execute build. A simple example:\n```cmake\ncmake_minimum_required (VERSION 2.8...3.21.1)\n\nproject(project_name)\n\ninclude_directories(${CMAKE_SOURCE_DIR}/include)\n\naux_source_directory(${CMAKE_SOURCE_DIR}/src DIR_SRC)\n\nadd_executable (project_name ${DIR_SRC})\n```\nThe variables and commands for `CMakeLists.txt` will be introduced in next part.\n\nNow we have all requirements for cmake, let's try:\n```bash\ncd build\ncmake ..\nmake\n```\nIf you don't have `make` installed, here is a short cut to make it work:\n1. Go to the folder where you install you compiler MinGW, in my case it's under `C:\\msys64\\mingw64\\bin`.\n2. Find a file named `mingw32-make.exe` and make a copy in the same foler.\n3. Rename the copy to `make.exe`.\n4. Check with `make --version` in cmd or powershell.\n\nIf the codes above work, you'll see *.exe created under `/build`, cmake works successfully.\n\n## Useful Variables and Commands for CMakeLists.txt\n----\nThe meaning of following words are able to be found under this [documentaion](https://cmake.org/cmake/help/latest/index.html)\n+ Variables: \n```cmake\nCMAKE_SOURCE_DIR\n\nPROJECT_SOURCE_DIR\n\nCMAKE_CURRENT_SOURCE_DIR\n\nCMAKE_MODULE_PATH\n\nEXECUTABLE_OUTPUT_PATH\n\nLIBRARY_OUTPUT_PATH\n\nPROJECT_NAME\n```\n+ Commands:\n```cmake\n// setup project name and supported progamming language\nproject(projectname [CXX] [C] [Java])\n\n// setup the supported cmake version \nCMAKE_MINIMUM_REQUIRED(VERSION versionNumber [FATAL_ERROR])\n\n// setup variables\nSET(VAR [VALUE] [CACHE TYPE DOCSTRING [FORCE]])\nset(EXECUTABLE_OUTPUT_PATH ${PROJECT_BINARY_DIR}/bin)\n\n// find all source code files and make them into a list\naux_source_directory(dir VARIABLE)\n\n// add subdirectory to the build\nadd_subdirectory(source_dir [binary_dir][EXCLUDE_FROM_ALL])\n\n// find head files\ninclude_directories([AFTER|BEFORE] [SYSTEM] dir1 dir2 ...)\n\n// Adds an executable target called <name> to be built from the source files listed\nadd_executable(<name> [source1] [source2 ...])\n\n// ...\n```\n## Reference:\n[1] [CMake Wiki](https://en.wikipedia.org/wiki/CMake) \\\n[2] [What are the benefits/purposes of cmake?](https://stackoverflow.com/questions/19686223/what-are-the-benefits-purposes-of-cmake) \\\n[3] [CMake Documentation](https://cmake.org/cmake/help/latest/index.html#)\n\n\n\n","tags":["CMake","VS Code"]},{"title":"How to Create Personal Blog on github Using Hexo","url":"/1970/01/01/personal_blogs/","content":"\n## Dependencies\n+ Node.js\n+ npm\n+ Git\n\n\n## Install Hexo\nif you don't have `node.js` and npm `npm` on your workstation, please visit [official site](https://nodejs.org/en/) to install the node.js. (npm would be automatically installed if node.js installation is done)\nTo check whether node.js and npm are successfully installed:\n```bash\n# in cmd/terminal\nnode -v # v14.17.3 or others\nnpm -v # 6.14.13 or others\n``` \nIf you are using win10 as your operate system and get errors with these codes, you should re-open the powershell/cmd as administor after the installation then try the codes above.\n\nThen:\n```bash\nnpm install -g hexo-cli\n```\nTo check whether hexo successfully installed:\n```bash\nhexo -v\n```\nYou'll see the version information e.g.:\n```bash\n hexo-cli:4.3.0, \n node:14.17.3, \n ... \n```\n\n## Create Blogs\nFirst of all, you have to navigate to the the folder, where you plan to create your blogs' folder.\n```bash\ncd E:\\\nmkdir my_blogs\ncd my_blogs\n```\nThen run:\n```bash\n# if you are using powershell as administor on windows,\nhexo init\n# if you are on mac/linux, \nsudo hexo init\n```\nCreate your first post:\n```bash\nhexo new \"My First Post\"\n```\nA new file with .md format will be created in `source/_posts/`. You could directly edit it in file. Also, you could preview your blog on your local machine with:\n```bash\n#establish a localhost for blog\nhexo server\n\n# if you have some updates to your blogs, use\nhexo clean\nhexo generate\nhexo server\n```\nThe response will show you a addresse like `http://localhost:4000`, you could visit it on local.\n\n\n## Deploy Blogs\nAfter you know how to write and make changes to your blogs on local, we'll introduce how to create a github page and deploy your blog to it. Install Deloyer with:\n```bash\nnpm install -save hexo-deployer-git\n``` \nGo to your github and create a new repositery called `username.github.io`. `username` should be your github username. This will be used as the address of github page later.\n\nOn your local, you need to edit the `_config.yml`. You could find this file in the blog folder. `_config.yml` is the key to set up your blogs, such as set up the name, discription, theme and so on. \n```bash\n# in _config.yml, find the part #Deployment change the info like below\ndeploy:\n  type: 'git'\n  repo: your-repo-adress\n  branch: master\n```\nAfter this, in your command line type:\n```bash\nhexo deploy\n```\nAfter a while, you would be able to visit `username.github.io` to see your own blog.\n\n## Change the Theme of Your Blog\nThere are varous free themes on [Hexo's official site](https://hexo.io/themes/). You could pick one to visit its github repo and follow the instruction to install it.\n\nE.g. I'd like to change the theme of my blog, firstly, \n```bash\n# in the path of your blog folder, clone the theme to local\ngit clone `url_of_the_theme_repo` themes/my_theme\n```\nThen modify the `_config.yml`. (most theme repos have demo, you could directly compare the `_config.yml` to your own and make some changes)\n\nAfter this:\n```bash\nhexo clean\nhexo generate\n# take a preview on local, \nhexo server\n# if everything is fine,\nhexo deploy\n```\n\n\n\nyou've successfully created your own blog! Congrats!"}]